[
  
    {
      "title"    : "Accelerate AI: VAC &amp; TAP in Action",
      "category" : "",
      "tags"     : "MLOPS",
      "url"      : "/post/2023/11/15/vac-tap-mlops.html",
      "date"     : "Nov 15, 2023",
      "content"  : "The Data Landscape and AI ProliferationIn a world where over 64.2 zettabytes of data are generated annually, the metaphor of traveling to the moon and back thousands of times if each byte were a kilometer vividly captures the enormity of data we‚Äôre generating. This data, sprawling across billions of devices and the Internet of Things (IoT), isn‚Äôt confined to a single location; it‚Äôs distributed across multiple clouds and mixed environments.With half a billion business data users worldwide, catering to a vast spectrum of needs and skills becomes imperative. However, it‚Äôs not just the storage that‚Äôs crucial; it‚Äôs the intelligent utilization of this data through AI and machine learning that empowers a billion workers. Yet, the road to implementing AI is laden with challenges.The Challenge of Resource Constraints and ComplexityThe stark reality is that only 1% of AI/ML projects fully achieve their objectives, underscoring a pressing challenge for the industry‚Äînot just in technology, but in strategy, support, and execution. No-code/low-code solutions emerge as a priority, with 96% of IT and engineering leaders acknowledging the acute shortage of software engineers. The complexity is another significant hurdle, with 72% of organizations still figuring out AI operationalization in their businesses.MLOps: A Beacon of Success in AIMLOps stands out as the strategic approach that could pivot more AI projects into the 1% success bracket. It‚Äôs about the right partners, strategy, and platform. For Tanzu Solution Engineers like us, it‚Äôs about leveraging the ‚ÄòDOO‚Äô framework, streamlining the app journey with TAP and the Golden Paths, optimizing app performance with our TKO bundle, and fostering continuous improvement with our Tanzu intelligence portfolio.MLOps is essentially about DevSecOps, but with a unique spin for managing AI and ML projects efficiently. It involves:  Preparing Data: Collecting, cleaning, and shaping data to train AI models effectively.  Building the Model: Experimenting with various models to find the best fit and ensure reliability.  Deploying, Consuming, and Monitoring: Integrating the model into applications, delivering business value, and ongoing performance monitoring.        MLOps isn‚Äôt a solo journey; it‚Äôs a collaborative team sport where each role is crucial:  Data Engineers lay down the robust data infrastructure.  Data Scientists craft predictive models.  Business Analysts ensure models meet market needs.  Application Developers integrate models into applications.  DevOps oversee the smooth operation of these applications.        Together, they turn data into actionable insights and drive business value, ensuring MLOps transcends model development‚Äîit becomes a comprehensive solution-delivery system.Our MLOps Demo: From Concept to ApplicationIn our demo, we‚Äôll illustrate this collaborative spirit in action. Julien will wear the dual hats of a Platform Engineer and Data Scientist‚Äîfirst deploying JupyterLab and MLflow, then creating a smart image-detection model. Subsequently, I will take Julien‚Äôs model and swiftly incorporate it into a Python accelerator as an App Developer.This is the essence of our MLOps approach‚Äîseamless integration of roles and tools, transforming innovative ideas into real-world applications with Tanzu‚Äôs suite of products.We will demonstrate just how efficiently and effectively we can harness the combined strength of JupyterHub, MLFlow, and TAP to bring AI models from development to deployment. For our demonstration we will VMware Application Catalog (VAC) and Tanzu Application Platform (TAP).        VMware Application Catalog (VAC)VAC is an enterprise solution that simplifies the utilization of open-source software in production environments. It offers a comprehensive catalog of tested open-source applications, with features like automated maintenance and vulnerability insights. This facilitates secure and compliant development processes.Tanzu Application Platform (TAP)TAP, a Platform as a Service (PaaS) solution, eases the deployment and management of cloud-native applications on Kubernetes. It enhances developer productivity and offers features like container orchestration, automation, and multi-cloud support.JupyterHubJupyterHub is a web-based platform that enables multiple users to collaboratively create and work with Jupyter notebooks on a shared server. It offers a secure and customizable environment, supports multiple users, and is commonly used in education, research, and data analysis for its collaborative and interactive capabilities.To install JupyterHub, use the following Helm command:helm install jupyterhub oci://harbor.jkolaric.eu/vac-library/charts/ubuntu-22/jupyterhubAfter installation, you can access JupyterHub using the following URL:export SERVICE_IP=$(kubectl get svc --namespace jupyter jupyterhub-proxy-public --template &quot;&quot;)echo &quot;JupyterHub URL: http://$SERVICE_IP/&quot;Admin user information:echo Admin user: userecho Password: $(kubectl get secret --namespace jupyter jupyterhub-hub -o jsonpath=&quot;{.data[&#39;values\.yaml&#39;]}&quot; | base64 -d | awk -F: &#39;/password/ {gsub(/[ \t]+/, &quot;&quot;, $2);print $2}&#39;)You can access Jupyter notebooks using a URL like this:http://20.67.149.113/user/user/lab/tree/opt/bitnami/jupyterhub-singleuser/Untitled.ipynbTest Jupyter Installation with a Deep Learning ModelThe following code essentially demonstrates how to use a pre-trained deep learning model (MobileNetV2) to classify the content of an image fetched from a given URL and visualize the prediction along with the image. You can copy/paste to your Jupyter and execute it.import requestsfrom PIL import Imageimport numpy as npfrom io import BytesIOimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictionsfrom tensorflow.keras.preprocessing.image import img_to_arrayimport os# T√©l√©charge une image depuis Internetdef download_image(image_url):    response = requests.get(image_url)    if response.status_code == 200:        img = Image.open(BytesIO(response.content))        return img    else:        return None# Pr√©dit le contenu de l&#39;imagedef predict_image(model, img):    img_resized = img.resize((224, 224))    img_array = img_to_array(img_resized)    img_array = np.expand_dims(img_array, axis=0)    img_array = preprocess_input(img_array)    predictions = model.predict(img_array)    return decode_predictions(predictions, top=1)[0][0]# URL de l&#39;imageimage_url = &#39;https://www.fklein.me/download/iphone2.jpg&#39;  # Remplacez avec l&#39;URL de l&#39;image que vous souhaitez analyser# Enregistrement du processus avec MLflow# T√©l√©charge et analyse l&#39;imageimg = download_image(image_url)if img is not None:    model = MobileNetV2(weights=&#39;imagenet&#39;)    prediction = predict_image(model, img)    # Affiche l&#39;image et la pr√©diction    plt.imshow(img)    plt.axis(&#39;off&#39;)    plt.title(f&quot;\nObject: {prediction[1]} \n\n Confiance in the prediction : {prediction[2]*100:.3f}%\n&quot;)    plt.show()else:    print(&quot;L&#39;image n&#39;a pas pu √™tre t√©l√©charg√©e.&quot;)Prerequisite for MLflowFor using MLflow, install the Python package:pip install mlflowRestart the kernel after installation in Jupyter UI.MLFlowMLflow is a tool that helps people who work with machine learning (ML) to do their work more easily. It helps with tracking and organizing ML experiments, packaging code, and deploying ML models. It‚Äôs useful for managing the entire ML process, from trying out ideas to putting models into real-world applications.To install MLflow, use the following Helm command:helm install mlflow oci://harbor.jkolaric.eu/vac-library/charts/redhatubi-8/mlflow -n mlflow --create-namespaceExpose the MLflow service:export SERVICE_IP=$(kubectl get svc --namespace mlflow mlflow-tracking --template &quot;&quot;)echo &quot;MLflow URL: http://$SERVICE_IP/&quot;Login credentials:echo Username: $(kubectl get secret --namespace mlflow mlflow-tracking -o jsonpath=&quot;{ .data.admin-user }&quot; | base64 -d)echo Password: $(kubectl get secret --namespace mlflow mlflow-tracking -o jsonpath=&quot;{.data.admin-password }&quot; | base64 -d)Use Jupyter to test the MLflow InstallationThe following code uses MLflow to track and log experiment information, metrics, and artifacts while performing image classification with a pre-trained MobileNetV2 model. It also saves the model and the downloaded image for later reference and displays the image with the predicted object class and confidence score.import requestsfrom PIL import Imageimport numpy as npfrom io import BytesIOimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictionsfrom tensorflow.keras.preprocessing.image import img_to_arrayimport mlflowimport osos.environ[&#39;MLFLOW_TRACKING_USERNAME&#39;] = &#39;user&#39;os.environ[&#39;MLFLOW_TRACKING_PASSWORD&#39;] = &#39;K1aLXzz0QW&#39;# Configuration de MLflowmlflow.set_tracking_uri(&#39;http://20.67.145.120:80&#39;)mlflow.set_experiment(&quot;image_classification_experiment&quot;)# T√©l√©charge une image depuis Internetdef download_image(image_url):    response = requests.get(image_url)    if response.status_code == 200:        img = Image.open(BytesIO(response.content))        return img    else:        return None# Pr√©dit le contenu de l&#39;imagedef predict_image(model, img):    img_resized = img.resize((224, 224))    img_array = img_to_array(img_resized)    img_array = np.expand_dims(img_array, axis=0)    img_array = preprocess_input(img_array)    predictions = model.predict(img_array)    return decode_predictions(predictions, top=1)[0][0]# URL de l&#39;imageimage_url = &#39;https://www.fklein.me/download/iphone2.jpg&#39;  # Remplacez avec l&#39;URL de l&#39;image que vous souhaitez analyser# Enregistrement du processus avec MLflowwith mlflow.start_run():    # T√©l√©charge et analyse l&#39;image    img = download_image(image_url)    if img is not None:        model = MobileNetV2(weights=&#39;imagenet&#39;)        prediction = predict_image(model, img)                # Log information        mlflow.log_param(&quot;image_url&quot;, image_url)        mlflow.log_metric(&quot;prediction_confidence&quot;, float(prediction[2]))        # Log l&#39;image        img.save(&quot;predicted_image.jpg&quot;)        mlflow.log_artifact(&quot;predicted_image.jpg&quot;)                # Log an instance of the trained model for later use        mlflow.tensorflow.log_model(model, artifact_path=&quot;object-detection&quot;)        # Affiche l&#39;image et la pr√©diction        plt.imshow(img)        plt.axis(&#39;off&#39;)        plt.title(f&quot;\nObject: {prediction[1]} \n\n Confiance in the prediction : {prediction[2]*100:.3f}%\n&quot;)        plt.show()    else:        print(&quot;L&#39;image n&#39;a pas pu √™tre t√©l√©charg√©e.&quot;)Image Classification Python Accelerator for TAP üêçüì∏The Python accelerator for TAP help you to deploy a serverless image classification function as a workload. The accelerator leverages the buildpacks provided by VMware‚Äôs open-source Function Buildpacks for Knative project.The accelerator includes the Python script that we execute before on Jupiter for image classification. It use the MobileNetV2 model and MLflow. It allows you to download an image from the internet, predict its contents, log the prediction and image in MLflow, and display the image with the prediction confidence. This serverless function can be easily integrated into your application or workflow.Prequesite :Have a Tanzu Application Platform installed.To add the accelerator to your Platform:tanzu acc create awesome-python-ai-image-function --git-repo https://github.com/fklein82/awesome-ai-python-function.git --git-branch main --intervalClone this repository to your local development environment:git clone &amp;lt;repository-url&amp;gt;cd python-accelerator-for-tanzuInside the python-function directory, you will find the func.py file. This Python function is invoked by default and serves as the entry point for your serverless image classification logic.python-function    ‚îî‚îÄ‚îÄ func.py // EDIT THIS FILEYou can customize the code inside this file to implement your specific image classification logic.If you want to explore more code samples for serverless functions that can be deployed within Tanzu Application Platform, you can check out the samples folder.Image Classification FunctionThe core functionality of this accelerator is the image classification function, which performs the following steps:  Downloads an image from a specified URL.  Predicts the content of the image using the MobileNetV2 model.  Logs the prediction and image in MLflow for tracking.  Displays the image with the prediction confidence.This function can be integrated into various applications and workflows that require image analysis and classification.DeploymentFor detailed instructions on how to build, deploy, and test your customized serverless image classification function using Tanzu Application Platform, please refer to the Tanzu website.To deploy this application on VMware Tanzu Application Platform, follow these steps:Ensure you have the Tanzu CLI installed and configured with access to your Tanzu Application Platform instance.Navigate to your project directory:cd [your-repo-directory]Use the Tanzu CLI to deploy your application:tanzu apps workload create -f config/workload.yamlMonitor the deployment status:tanzu apps workload tail awesome-python-ai-image-function --timestamp --since 1hOnce deployed, access your application via the URL provided by Tanzu Application Platform. You can find the URL with the following command:tanzu apps workload get awesome-python-ai-image-functionConclusionIn this  guide, we‚Äôve covered a wide range of topics related to MLOps (Machine Learning Operations), DevOps, JupyterHub, MLflow, and the integration of various technologies, including VMware Application Catalog (VAC) and Tanzu Application Platform (TAP). Let‚Äôs summarize the key takeaways:MLOps Demo with VAC and TAPThis guide has provided detailed instructions for setting up JupyterHub and MLflow on your Kubernetes (K8s) system, leveraging the power of VMware Application Catalog (VAC). The objective is to enable you to create an impressive MLOps demonstration and showcase AI applications deployed on Tanzu Application Platform (TAP), a robust Platform as a Service (PaaS) solution running seamlessly on top of Kubernetes.With TAP, you can effortlessly deploy AI applications that utilize a Machine Learning API exposed by Kubeflow. Whether you‚Äôre an AI enthusiast, data scientist, or a tech enthusiast, this guide empowers you to harness the potential of K8s, VAC, and TAP for an awe-inspiring AI journey.VMware Tanzu Application Catalog simplifies and secures the use of open-source software components for production. It offers a diverse catalog of rigorously tested open-source applications, automated maintenance, vulnerability insights, and more. This streamlines development while ensuring security and compliance. Tanzu Application Catalog is the enterprise version of the open-source Bitnami Application Catalog, providing stronger control and visibility into open-source software supply chains.VMware Tanzu Application Platform is a platform-as-a-service (PaaS) solution that simplifies the deployment and management of cloud-native applications and microservices in a Kubernetes environment. It offers developer productivity, container orchestration, self-service deployment, automation, monitoring, multi-cloud support, and security features.MLOps vs DevOpsMLOps and DevOps are related concepts focused on streamlining and automating software development and deployment processes, but they have different areas of application and emphasis:      DevOps (Development and Operations): DevOps aims to unify software development and IT operations teams, focusing on improving collaboration, automation, and the software development lifecycle. Its goal is to shorten development cycles and enhance the quality of deployments.        MLOps (Machine Learning Operations): MLOps is an extension of DevOps tailored specifically for machine learning and AI projects. It covers the end-to-end process of developing, deploying, and managing machine learning models in production. The primary goal is to streamline and automate the ML lifecycle.  Key differences include scope, focus on data, the model lifecycle, and specialized tools and technologies used in MLOps.JupyterHubJupyterHub is a collaborative, web-based platform that allows multiple users to create and work with Jupyter notebooks on a shared server. It offers a secure and customizable environment, making it ideal for education, research, and data analysis.MLflowMLflow is a versatile tool for tracking, organizing, packaging, and deploying machine learning experiments and models. It simplifies the entire ML process, from experimenting with ideas to deploying models in real-world applications. The guide includes instructions for installing and using MLflow in your environment.To illustrate the capabilities of JupyterHub and MLflow, a code example was provided for image classification using a pre-trained MobileNetV2 model. This code showcases how to download an image, make predictions, log experiment information, metrics, and artifacts using MLflow, and display the image with predictions.Before using MLflow, don‚Äôt forget to install the Python package using pip install mlflow and restart the Jupyter kernel if necessary.In a nutshellThis guide give you the knowledge and tools needed to set up a powerful MLOps environment using VAC, TAP, JupyterHub, and MLflow. It empowers you to explore the exciting world of AI and machine learning with confidence, whether you‚Äôre a data scientist, developer, or IT professional.AuthorsThis blog post was co-written with my friend Julien Kolaric.Linkedin :  Julien Kolaric  Fr√©d√©ric Klein             ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-11-15T12:00:35+01:00'>15 Nov 2023</time><a class='article__image' href='/post/2023/11/15/vac-tap-mlops.html'> <img src='/images/mclaren.png' alt='Accelerate AI: VAC &amp; TAP in Action'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/11/15/vac-tap-mlops.html'>Accelerate AI: VAC &amp; TAP in Action</a> </h2><p class='article__excerpt'>Discover the power of MLOps with Tanzu&#39;s VMware Application Catalog (VAC) and Tanzu Application Platform (TAP). Build a demo that can transform complex data into cutting-edge AI applications, illustrating the seamless journey from data engineering to model deployment and beyond.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Discover How to Integrate Tanzu Application Platform with Github Actions: A Step-by-Step Guide.",
      "category" : "",
      "tags"     : "CI/CD",
      "url"      : "/post/2023/07/18/tap-github-action.html",
      "date"     : "Jul 18, 2023",
      "content"  : "In modern software development practices, the Continuous Integration/Continuous Delivery (CI/CD) pipeline plays a vital role. It automates the process of integrating code changes and delivering the product to the production environment. In this post, we‚Äôll look at how we can integrate an existing Github Actions CI pipeline with the Tanzu Application Platform for seamless application delivery on a Kubernetes environment.Our ObjectivesThe use case we‚Äôll be exploring involves a Jekyll blog, a static website crafted with Jekyll‚Äôs static site generator. Our objective is threefold:      Automate the Docker Image Creation: We aim to automate the process of constructing a Docker image from the Jekyll blog.        Push Docker Image to Registries: Following the creation, the Docker image should be automatically pushed to both Docker Hub and a personal Harbor registry upon any changes to the blog.        Deploy and Expose the Blog with TAP: Finally, we aim to utilize Tanzu Application Platform to deploy and expose the Jekyll blog on a Kubernetes cluster.  In order to achieve these objectives, we will set up a Dockerfile, configure a Github Actions workflow, and configure the Tanzu Application Platform.        What is Github &amp;amp; TAP ?      Github is a web-based platform for software development using Git. It allows developers to collaborate, track changes, and manage code repositories. It offers features like version control, issue tracking, code review, and CI/CD automation through GitHub Actions. GitHub simplifies collaboration and helps developers work together effectively on projects.        Tanzu Application Platform or TAP, is a cloud-native application platform developed by VMware Tanzu. It simplifies the deployment and management of applications on Kubernetes clusters, providing developers with an easier way to deploy and run their applications at scale. TAP automates the creation and management of Kubernetes resources, allowing developers to focus on writing code rather than dealing with infrastructure complexities.  Leveraging Existing CI/CD Pipelines with TanzuMany organizations today have already invested significant effort into creating CI/CD pipelines that work well for their needs. In our case, we‚Äôre looking at a scenario where we already have a Github Actions CI pipeline in place. The question is, how can we extend this to leverage Tanzu Application Platform‚Äôs capabilities for the CD part of our pipeline?The Tanzu Application Platform is designed to simplify the process of deploying and managing applications on Kubernetes. It‚Äôs built with the understanding that developers and operators need a simplified, more effective way to build and run modern, cloud-native applications. By integrating Tanzu into your existing pipeline, it can take over the complexities of networking, scaling, and operational management, providing you a more effective way to build and run modern, cloud-native applications        If you already have a Github Actions pipeline for building and pushing Docker images (the CI part), you can easily integrate this with Tanzu Application Platform for the deployment part (CD). The built Docker images will be pushed to Docker Hub or your personal Harbor registry and Tanzu will handle the deployment of these images into a Kubernetes environment.        Integrating Tanzu with Github Actions - What we will do?As we dive into this integration process, it‚Äôs important to clarify what we‚Äôre trying to achieve. In this particular scenario, we will be dealing with my personal Jekyll blog (blog.fklein.me). Jekyll is a popular static site generator that takes Markdown files and converts them into a complete static website. Our main goal here is to build a Docker image from this Jekyll blog, and then push this image to two different container registries: Docker Hub and a personal Harbor registry.Step 1: Configure Github Actions ‚Äì Set up Github Actions to automate tasks like building an docker image and push it to a registry.  To be able to build a Docker image from our Jekyll blog, we will need a Dockerfile in our project. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.This is an example of a Dockerfile to run the Jekyll Blog:# Container image that runs your codeFROM php:8.2-apache# Copies your code file from your action repository to the filesystem path `/` of the containerCOPY /_site/ /var/www/html/The source code is available on my github repo: https://github.com/fklein82/cloud-server  Once our Dockerfile is set up and our Jekyll blog is ready to be containerized, we‚Äôll use Github Actions to automate the Build process. Each time a change is pushed to our Github repository (which could be a new blog post, an update to an existing post, a layout change, etc.), Github Actions will trigger our workflow.Our workflow is defined in the YAML file in the .github/workflows directory. It starts by checking out the source code from our repository, and then it logs into Docker Hub using the credentials we‚Äôve stored as secrets in our Github repository.Next, it retrieves the current date and time, which we‚Äôll use to tag our Docker image. The Docker image is then built from our Dockerfile and tagged with the current date.The workflow then pushes the newly built Docker image to Docker Hub, allowing it to be pulled and run on any Docker-enabled system that can access Docker Hub.In the next steps, the workflow logs into a personal Harbor registry. Harbor is an open-source cloud-native registry that stores, signs, and scans container images for vulnerabilities. Just like with Docker Hub, the workflow then pushes our Docker image to the Harbor registry.With this workflow, not only are we able to keep our Docker image up-to-date on Docker Hub and our personal Harbor registry automatically, but we can also ensure that our Jekyll blog is quickly and consistently deployed each time we make a change.This is the : .github/workflows/push-docker-image.yaml filename: Build-Push-DockerHUB-Harboron: [push] # When pushing to any branch then run this action# Env variableenv:  DOCKER_USER: $  DOCKER_PASSWORD: $  REPO_NAME: $  HARBOR_USER: $  HARBOR_PASSWORD: $jobs:  push-image-to-docker-hub:  # job name    runs-on: ubuntu-latest  # runner name : (ubuntu latest version)     steps:    - uses: actions/checkout@v2 # first action : checkout source code    - name: DockerHub login      run: | # log into docker hub account        docker login -u $DOCKER_USER -p $DOCKER_PASSWORD      - name: Get current date # get the date of the build      id: date      run: echo &quot;::set-output name=date::$(date +&#39;%Y-%m-%d-%H-%M-%S&#39;)&quot;    - name: Build the Docker image # push The image to the docker hub      run: docker build . --file Dockerfile --tag $DOCKER_USER/$REPO_NAME:$ --tag registry.fklein.me/tanzu-blog/fklein-blog:$    - name: DockerHub Push      run: docker push $DOCKER_USER/$REPO_NAME:$    - name: Fklein Harbor login      run: | # log into Personal Harbor account        docker login registry.fklein.me -u $HARBOR_USER -p $HARBOR_PASSWORD      - name: fklein harbor Push      run: docker push registry.fklein.me/tanzu-blog/fklein-blog:$Step 2: Unleashing Tanzu Application Platform for DeploymentAfter your Github Actions workflow successfully builds and pushes the Docker image of your Jekyll blog to Docker Hub or your personal Harbor registry, it‚Äôs time for Tanzu Application Platform (TAP) to take the lead. In our case, we have deployed TAP on top of AKS (Azure Kubernetes Service), which provides a robust and scalable Kubernetes cluster.TAP simplifies the deployment process by automating the creation and management of Kubernetes resources, such as deployments, services, and pods. With TAP on AKS, you get the benefits of both Tanzu‚Äôs streamlined deployment experience and AKS‚Äôs reliable and scalable infrastructure.To deploy your Jekyll blog using TAP on AKS, you can utilize the Tanzu CLI with the following command:tanzu apps workload create blog \  --type web \  --label app.kubernetes.io/part-of=blog \  --image registry.fklein.me/tanzu-blog/fklein-blog:2023-07-18-14-23-00This command is an instruction set to Tanzu to orchestrate the creation of a new workload for your blog. It includes essential configurations for the deployment - the type of the workload, any necessary labels or annotations, and crucially, the Docker image to use. Once this information is processed, Tanzu swings into action, deploying this image seamlessly into your Kubernetes environment.        The integration of Tanzu in this process not only offloads the complexity of deployment from your team but also ensures an efficient, scalable and robust solution, leaving you free to focus on your core application development tasks.        But that‚Äôs not all‚ÄîTAP goes a step further by integrating continuous deployment and security testing into the supply chain.        With the continuous deployment pipeline configured in Tanzu, you can seamlessly integrate security testing steps before deploying your applications. These steps can include vulnerability scanning and compliance checks to ensure your deployments meet the required security standards. This helps you deliver high-quality and secure applications to your users.        ConclusionIncorporating Tanzu Application Platform with Github Actions allows you to extend your existing CI pipelines and benefit from Tanzu‚Äôs powerful application management capabilities. This setup enables automated and consistent deployments of your applications into a Kubernetes environment, eliminating the need for complex YAML files and streamlining the deployment process.By integrating Tanzu, you can leverage its higher-level abstractions and standardized components to simplify the deployment of your applications on Kubernetes. Tanzu handles the creation and management of Kubernetes resources, such as deployments, services, and pods, so you can focus on writing code rather than dealing with infrastructure complexities.        This seamless integration between Tanzu Application Platform and Github Actions showcases Tanzu‚Äôs flexibility and its ability to seamlessly integrate with pre-existing workflows. You can continue to leverage the power of Github Actions for your CI pipeline, while Tanzu takes care of the deployment process on Kubernetes.By leveraging Tanzu Application Platform, you save time and ensure repeatability in your deployment processes. The automation provided by Tanzu eliminates manual steps and reduces the risk of errors, leading to more efficient and consistent deployments.In conclusion, Tanzu Application Platform is an excellent choice for organizations looking to optimize their Kubernetes application deployments. By integrating it with Github Actions, you can enhance your CI pipeline, simplify the deployment process, and enjoy the benefits of automated and consistent deployments into a Kubernetes environment, all without the need for complex YAML files.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-07-18T13:00:35+02:00'>18 Jul 2023</time><a class='article__image' href='/post/2023/07/18/tap-github-action.html'> <img src='/images/tap-github-action.png' alt='Discover How to Integrate Tanzu Application Platform with Github Actions: A Step-by-Step Guide.'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/07/18/tap-github-action.html'>Discover How to Integrate Tanzu Application Platform with Github Actions: A Step-by-Step Guide.</a> </h2><p class='article__excerpt'>Explore the powerful synergy of Github Actions and Tanzu Application Platform (TAP) in our latest blog post. Learn to automate Docker image creation from a Jekyll blog, push it to Docker Hub and a personal Harbor registry, and deploy it using TAP in a Kubernetes environment. The post simplifies the complexities of Kubernetes deployment, allowing you to focus on coding while improving your CI/CD pipeline efficiency.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Setup your DATA &amp; IA Platform with Tanzu - A Real-World Guide Machine Learning use-case.",
      "category" : "",
      "tags"     : "data-ia",
      "url"      : "/post/2023/07/12/tap-greenplum-python.html",
      "date"     : "Jul 12, 2023",
      "content"  : "Building an MLOps Platform with Tanzu Application Platform and GreenplumAs a team of data scientists and engineers at VMware Tanzu, we‚Äôve been exploring how we can leverage Tanzu Application Platform (TAP) and Greenplum to build a comprehensive MLOps platform. In this blog post, we‚Äôll explain how to set up such a platform and provide a practical example of machine learning in action, predicting the age of abalones using linear regression.Tanzu Application Platform and GreenplumFor this case study, we are working with a system that uses both the Tanzu Application Platform (TAP) and VMware Greenplum, both of which are deployed on the Azure Cloud platform.              Azure Cloud is Microsoft‚Äôs public cloud computing platform. It provides a range of cloud services, including those for computing, analytics, storage, and networking. Users can pick and choose from these services to develop and scale new applications, or run existing applications, in the public cloud.        Tanzu Application Platform (TAP) is a part of VMware Tanzu. It is designed to make it easier for developers to build, deploy, and manage applications on Kubernetes. In our case, TAP is deployed on the Azure Kubernetes Service (AKS), which is a managed container orchestration service provided by Azure. AKS simplifies the deployment, scaling, and operations of Kubernetes, thereby allowing TAP to fully utilize its modular capabilities for modern applications.        VMware Greenplum is a high-performance, massively parallel data warehouse that provides powerful and rapid analytics on petabyte-scale data volumes. In our setup, Greenplum is deployed on top of virtual machines (VMs) on Azure Cloud. These VMs can be easily scaled and managed within the Azure ecosystem, allowing for the efficient handling of large data workloads by Greenplum.  So, our platform foundation is a Kubernetes cluster hosted on Azure‚Äôs AKS. This cluster is used to run TAP, which supports the development and management of our modern applications. Concurrently, we use Greenplum on Azure VMs to provide robust analytics on large-scale data. This setup provides us with a scalable, efficient, and powerful platform for both application development and data analytics.Before we get started, it‚Äôs important to note that we assume you already have a Kubernetes cluster with TAP installed. In this guide, we have deployed a TAP platform on AKS by following the instructions here.Next, we installed Greenplum 7 along with its data science Python packages which you can learn more about here and here.Jupyter Lab and AcceleratorsJupyterLab is an interactive development environment for working with notebooks, code and data. It provides the ability to execute code in a number of programming languages and to organize that code along with narrative text, equations, images, and visualizations in a single document.On the other hand, an accelerator for Tanzu Application Platform (TAP) is a bit of software that aids in speeding up the development and deployment process of applications on TAP. Accelerators provide pre-configured templates or a set of scripts that automate the generation of code, configuration, and other operational aspects, enabling developers to focus on coding rather than configuration.In this case, we‚Äôll use a JupyterLab accelerator available here.You can add the accelerator to Tanzu Application Platform List by executing the following code:tanzu acc create jupyter-lab --git-repo https://github.com/fklein82/jupyter-lab-for-tap --git-branch main --interval 5sThen you can deploy Jupyter-LAB by generate the acceleraror on your local machine and execting the following commands:tanzu apps workload create -f $DIR/config/workload.yamlAnd this will deploy Jupyter-LAB on TAP:        You can see the Pod running on AKS and the url with the TAP UI:        This is the UI of Jupyter-LAB deployed on our Tanzu Application Platform:        MLflowMLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, and deployment. It integrates with any Python, R, or Java-based Machine Learning algorithm and simplifies the process of tracking experiments, packaging code into reproducible runs, and sharing and deploying models.For MLflow, we used the Accelerator from our colleagues Omotola Oawofolu, and it can be found here.You can add the accelerator to Tanzu Application Platform List by executing the following code:tanzu acc create jupyter-lab --git-repo --git-repository https://github.com/agapebondservant/mlflow-accelerator --git-branch main --interval 5sThis is the UI of MLflow deployed on our Tanzu Application Platform:        Python Script for Abalone Age PredictionWith our MLOps platform ready, we‚Äôre set to tackle a real-world machine learning use case: predicting the age of abalones using linear regression. Let‚Äôs dive into the Python script.The script uses Greenplum‚Äôs ability to directly access external data to read the abalone dataset from an online source. The script is also utilizing sql_magic, an IPython extension, to write SQL queries to run against Greenplum database, as well as the GreenplumPython library for manipulating Greenplum data with Python.1. Environment setup:The script starts by installing the required packages and importing them. These include Python libraries such as pandas, numpy, plotly, and sqlalchemy for data manipulation, analysis, and visualization; greenplumpython for connecting to and interacting with the Greenplum database; and SQL magic commands that allow running SQL queries in Jupyter notebooks. It also sets up the database connection to Greenplum.## Setup environment: install &amp;amp; import packages!pip install greenplum-python pandas numpy plotly ipython-sql sqlalchemy plotly-express sql_magic pgspecialimport pandas as pdimport numpy as npimport osimport sysimport plotly_express as px# For DB Connectionfrom sqlalchemy import create_engineimport psycopg2import pandas.io.sql as psqlimport sql_magicimport greenplumpython as gpimport plotly.io as piopio.renderers.default = &#39;iframe&#39;2. Data Collection:The script creates an external web table that pulls in the abalone data directly from an online resource using Greenplum‚Äôs CREATE EXTERNAL WEB TABLE statement. The external web table data is then transferred to a regular Greenplum table.### Database Connection%load_ext sql%sql postgresql://gpadmin:password@xxx.xxx.xxx.xxx/warehouse%%sql SELECT version ();# Data Collection: Access external web data directly from Greenplum%%sql-- External TableDROP EXTERNAL TABLE IF EXISTS abalone_external;CREATE EXTERNAL WEB TABLE abalone_external(    sex text    , length float8    , diameter float8    , height float8    , whole_weight float8    , shucked_weight float8    , viscera_weight float8    , shell_weight float8    , rings integer -- target variable to predict) EXECUTE &#39;curl http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data&#39;format &#39;CSV&#39;(null as &#39;?&#39;);%%sql-- Create abalone table from an external tableDROP TABLE IF EXISTS abalone;CREATE TABLE abalone AS (    SELECT ROW_NUMBER() OVER() AS id, *    FROM abalone_external) DISTRIBUTED BY (sex);3. Exploratory Data Analysis (EDA):Basic SQL queries and the GreenplumPython library are used to inspect the data and get a sense of what it contains. This part also uses Plotly to visualize the distribution of the ‚Äúsex‚Äù category.# Exploratory Data Analysis### Inspect the table using basic SQL%%sql SELECT * FROM abalone LIMIT 10;        ### Inspect the table using GreenplumPython library# GreenplumPython connection to DBdb = gp.database(&quot;postgresql://gpadmin:password@xxx.xxx.xxx.xxx/warehouse&quot;)abalone = db.create_dataframe(table_name=&quot;abalone&quot;)# SELECT * FROM abalone ORDER BY id LIMIT 10;abalone.order_by(&quot;id&quot;)[:10]        #### Row-count of the &quot;abalone&quot; table# SELECT gp_segment_id, COUNT(*)# FROM abaloneimport greenplumpython.builtins.functions as Fabalone.apply(lambda _: F.count())count66832### Distribution of &quot;sex&quot; in abalone datasetgroup_by_sex = abalone.group_by(&quot;sex&quot;).apply(lambda _: F.count())df_group_by_sex = pd.DataFrame.from_records(iter(group_by_sex))df_group_by_sex        px.pie(df_group_by_sex, names = &#39;sex&#39;, values = &#39;count&#39;, title=&#39;Distribution of sex categories&#39;)        4. Feature Engineering:The dataset is split into a training set and a test set using SQL queries.%%sqlCREATE TEMP TABLE temp_abalone_label AS    (SELECT *, random() AS __samp_out_label FROM abalone);CREATE TEMP TABLE train_percentile_disc AS    (SELECT sex, percentile_disc(0.8) within GROUP (ORDER BY __samp_out_label) AS __samp_out_label    FROM temp_abalone_label GROUP BY sex);CREATE TEMP TABLE test_percentile_disc AS    (SELECT sex, percentile_disc(0.2) within GROUP (ORDER BY __samp_out_label) AS __samp_out_label    FROM temp_abalone_label GROUP BY sex);DROP TABLE IF EXISTS abalone_train;CREATE TABLE abalone_train AS    (SELECT temp_abalone_label.*        FROM temp_abalone_label        INNER JOIN train_percentile_disc        ON temp_abalone_label.__samp_out_label &amp;lt;= train_percentile_disc.__samp_out_label        AND temp_abalone_label.sex = train_percentile_disc.sex    );DROP TABLE IF EXISTS abalone_test;CREATE TABLE abalone_test AS    (SELECT temp_abalone_label.*        FROM temp_abalone_label        INNER JOIN test_percentile_disc        ON temp_abalone_label.__samp_out_label &amp;lt;= test_percentile_disc.__samp_out_label        AND temp_abalone_label.sex = test_percentile_disc.sex    )Command result:66832 rows affected.3 rows affected.3 rows affected.Done.53467 rows affected.Done.13368 rows affected.5. In-database Machine Learning:The main function is defined. It uses the GreenplumPython library to load the train and test datasets from the database. The function linreg_func is created, which takes in three lists (length, shucked weight, and rings) and returns a data class LinregType. Inside this function, the linear regression model is trained on the length and shucked weight, and the model is serialized. The model is logged to MLFlow, and the model‚Äôs coefficients, intercept, and metadata are returned.import greenplumpython as gpfrom typing import Listimport dataclassesdef main():    db = gp.database(&quot;postgresql://gpadmin:password@xxx.xxx.xxx.xxx/warehouse&quot;)    abalone = db.create_dataframe(table_name=&quot;abalone&quot;)    import greenplumpython.builtins.functions as F    abalone_train = db.create_dataframe(table_name=&quot;abalone_train&quot;)    abalone_test = db.create_dataframe(table_name=&quot;abalone_test&quot;)    print(abalone_test[:1])    print(abalone_train[:1])    # -- Create function    # -- Need to specify the return type -&amp;gt; API will create the corresponding type in Greenplum to return a row    # -- Will add argument to change language extensions, currently plpython3u by default    @dataclasses.dataclass    class LinregType:        model_name: str        col_nm: List[str]        coef: List[float]        intercept: float        serialized_linreg_model: bytes        created_dt: str        run_id: str        registered_model_name: str        registered_model_version: str    @gp.create_column_function    def linreg_func(length: List[float], shucked_weight: List[float], rings: List[int]) -&amp;gt; LinregType:        from typing import List        import dataclasses        from sklearn.linear_model import LinearRegression        import numpy as np        import pickle        import mlflow        import datetime        import os        os.environ[&quot;AZURE_STORAGE_ACCESS_KEY&quot;] = &quot;XXX&quot;        os.environ[&quot;AZURE_STORAGE_CONNECTION_STRING&quot;] = &quot;DefaultEndpointsProtocol=https;AccountName=XXX;AccountKey=XXX;EndpointSuffix=core.windows.net&quot;        @dataclasses.dataclass        class LinregType:            model_name: str            col_nm: List[str]            coef: List[float]            intercept: float            serialized_linreg_model: bytes            created_dt: str            run_id: str            registered_model_name: str            registered_model_version: str        mlflow.set_tracking_uri(&quot;http://20.93.3.160:5000&quot;)        mlflow.set_experiment(&#39;test&#39;)        experiment = mlflow.get_experiment_by_name(&#39;test&#39;)        experiment_id = experiment.experiment_id        mlflow.autolog()        with mlflow.start_run(experiment_id=experiment_id,nested=True) as run:            model_name=&quot;model_greenplum&quot;            mlflow.log_param(&quot;start_run_test&quot;, &quot;This is a test&quot;)            X = np.array([length, shucked_weight]).T            y = np.array([rings]).T            # OLS linear regression with length, shucked_weight            linreg_fit = LinearRegression().fit(X, y)            linreg_coef = linreg_fit.coef_            linreg_intercept = linreg_fit.intercept_            mlflow.log_param(&quot;start_run_test2&quot;, &quot;This is a test 2&quot;)            # Serialization of the fitted model            serialized_linreg_model = pickle.dumps(linreg_fit, protocol=3)            mlflow.sklearn.log_model(linreg_fit, model_name)            # Register the model to MLFlow            model_uri = &quot;runs:/{}/model&quot;.format(run.info.run_id)            mv = mlflow.register_model(model_uri, model_name)            mlflow.sklearn.log_model(                    sk_model=linreg_fit,                    artifact_path=&quot;model&quot;,                    registered_model_name=model_name,                )            return LinregType(                model_name=model_name,                col_nm=[&quot;length&quot;, &quot;shucked_weight&quot;],                coef=linreg_coef[0],                intercept=linreg_intercept[0],                serialized_linreg_model=serialized_linreg_model,                created_dt=str(datetime.datetime.now()),                run_id=str(run.info.run_id),                registered_model_name=str(mv.name),                registered_model_version=str(mv.version)            )    linreg_fitted = (        abalone_train.group_by()        .apply(lambda t: linreg_func(t[&quot;length&quot;], t[&quot;shucked_weight&quot;], t[&quot;rings&quot;]), expand=True)    )    print(linreg_fitted[[&quot;model_name&quot;, &quot;col_nm&quot;, &quot;coef&quot;, &quot;intercept&quot;, &quot;created_dt&quot;, &quot;run_id&quot;, &quot;registered_model_name&quot;,                   &quot;registered_model_version&quot;]])    linreg_test_fit = linreg_fitted.cross_join(        abalone_test,        self_columns=[&quot;col_nm&quot;, &quot;coef&quot;, &quot;intercept&quot;, &quot;serialized_linreg_model&quot;, &quot;created_dt&quot;, &quot;registered_model_name&quot;,                      &quot;registered_model_version&quot;]    )    print(linreg_test_fit[:1])6. Model Training:The linear regression function is applied to the training data using the group_by().apply() method from the GreenplumPython library.7. Model Testing:The test data is combined with the trained model using the cross_join() method, and predictions can be made based on the trained model.In this script, Greenplum‚Äôs power is used to perform in-database machine learning. This allows processing large amounts of data without moving it out of the database, leading to improved performance.8. Integration with MLflow:The script also showcases the integration with MLflow for model tracking and versioning. Once you‚Äôve run some experiments with MLflow, you can go to its web interface to see an overview of all your experiments, each one with a unique name, start time, user, and other useful metadata.By clicking on a specific run, you can see more detailed information including the input parameters, output metrics, tags, and any notes you may have added. You can also visualize the model‚Äôs performance over time and across different parameters. Additionally, MLflow allows you to store the model for each run. You can compare different runs, revert to older models, or deploy the model directly from MLflow.        This script is used to predict the age of abalones (represented by the ‚Äúrings‚Äù column in the dataset) using a linear regression model trained on two features: the length and shucked weight of the abalones. It‚Äôs an example of supervised learning as it uses labelled data (i.e., we know the actual age of the abalones in the training set).The data for this example comes from the UCI Machine Learning Repository. It‚Äôs a well-known dataset in the machine learning community, often used to illustrate various data analysis and machine learning techniques. In this case, the dataset provides a practical use case for the Tanzu Application Platform and Greenplum capabilities in setting up an MLOps environment.Conclusion:Machine learning and data analysis are becoming increasingly vital in the modern data-driven world. Tools like the Tanzu Application Platform and Greenplum enable you to leverage the power of in-database machine learning to handle large volumes of data effectively and efficiently. By applying these tools to real-world datasets, like the Abalone dataset from the UCI Machine Learning Repository, we‚Äôre able to see just how powerful and practical these technologies can be.The script showcased in this blog post takes advantage of the in-database processing capabilities of Greenplum, demonstrating that you can build and test machine learning models without moving your data out of the database. This not only enhances performance but also adds a layer of security, as the data remains within its original environment.The integration with MLflow provides invaluable assistance in managing our machine learning lifecycle. It helps keep track of various model versions, logs all relevant metrics, parameters, and even notes, ensuring an organized and transparent machine learning process. With its visual interface, it becomes easier to compare different model runs, deploy the model, or revert to older models, thus enabling robust and reproducible machine learning.In the grand scheme of MLOps, the combination of Greenplum for in-database machine learning and MLflow for model tracking and versioning provides a powerful and efficient solution. This empowers data scientists and engineers to perform more complex analyses, develop more sophisticated models, and ultimately extract more valuable insights from their data. As the field of machine learning continues to evolve, these tools will undoubtedly play an integral role in shaping its future.Thank you for joining us in this exploration of Greenplum and MLflow. I hope this post has helped illustrate their potential and inspires you to consider how they could enhance your own data science projects. Stay tuned for more insights and tutorials in machine learning and data science. Happy coding!AuthorsThis blog post was co-written with my friends Ruxue Zeng and Ahmed Rachid Hazourli.Linkedin :  Ruxue Zeng  Ahmed Rachid Hazourli  Fr√©d√©ric Klein                 We sincerely hope you enjoyed reading it!",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-07-12T18:01:35+02:00'>12 Jul 2023</time><a class='article__image' href='/post/2023/07/12/tap-greenplum-python.html'> <img src='/images/data-ia.png' alt='Setup your DATA &amp; IA Platform with Tanzu - A Real-World Guide Machine Learning use-case.'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/07/12/tap-greenplum-python.html'>Setup your DATA &amp; IA Platform with Tanzu - A Real-World Guide Machine Learning use-case.</a> </h2><p class='article__excerpt'>In this blog post, we walk you through the process of setting up the DATA-IA platform with Tanzu Application Platform and Greenplum Database. We leverage these technologies to build and deploy a Machine Learning model that predicts the age of abalone, providing a practical use-case to help you understand the capabilities of these robust data platforms.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Discover the power of MLOps with Tanzu Application Platform (TAP) and Greenplum.",
      "category" : "",
      "tags"     : "data-ia",
      "url"      : "/post/2023/05/08/tap-and-greenplum.html",
      "date"     : "May 8, 2023",
      "content"  : "Introduction to Machine Learning, Artificial Intelligence and Data Platforms.In the world of data, companies use Machine Learning (ML) and Artificial Intelligence (AI) to stay competitive. As the demand for quick innovation and deployment of ML models increases, having a strong all-in-one data platform becomes crucial.In this blog post, we will explore how combining the Tanzu Application Platform (TAP) with Greenplum can deliver a full data platform with MLOps capabilities by using Opensource projects.      Tanzu Application Platform (TAP) is a ‚ÄúPlatform as a Service‚Äù that simplifies the development, deployment, and management of modern applications on Kubernetes.        Tanzu Application Platform can easily integrate with just about any modern database using Service Bindings. This includes databases with support for in-database analytics. In this blog post, we will use VMware Greenplum.        VMware Greenplum is an advanced, fully featured, open-source MPP data warehouse based on PostgreSQL. It provides powerful and rapid analytics on petabyte-scale data volumes. Uniquely geared toward big data analytics.          What is MLOps? Unlocking the Secrets to Efficient Machine Learning Development and DeploymentMLOps, aka Machine Learning Operations, is a set of practices that aim to streamline the development, deployment, and management of machine learning models. It involves integrating machine learning with DevOps principles to ensure smooth collaboration between data scientists, ML engineers, and IT operations teams. MLOps focuses on automating and monitoring various stages of the ML lifecycle, from data preprocessing to model deployment and maintenance, resulting in faster experimentation, improved model quality, and more reliable production systems.        Why Greenplum for the Back-end?Exporting data from a database and importing it into a server or desktop environment using popular data science tools (e.g., Python, R) can be inefficient for big data analytics. Data scientists often face challenges with these tools‚Äô memory and scalability limitations as well as bottlenecks associated with transferring large amounts of data between different platforms.Choosing the right tool is critical for data scientists to overcome these issues. In this post, we focus on Greenplum, a massively parallel processing PostgreSQL engine which provides built-in tools for data scientists for high-scale data exploration and model training. These tools and extensions include:  Procedural language extensions to enable massive parallelism for Python &amp;amp; R  Apache MADlib for scalable machine learning  PostGIS for geospatial analytics and GPText for text search and processing  Interoperability with dashboarding tools such as Tableau and PowerBI for seamless data visualization and reporting            GreenplumPython for End-To-End MLOps tasks with MLflowOur JourneyOur journey will take us through the process of training a Convolutional Neural Network (CNN) on TAP, discovering data sets with DataHub, setting up a development environment, building ML workflows with Kubeflow and Argo Workflows, and creating predictive apps with APIs.But what is a Convolutional Neural Network?A Convolutional Neural Network (CNN) is a type of computer program designed to process and analyze grid-like data, such as images. It‚Äôs especially good at tasks like recognizing and classifying objects in pictures. CNNs work by learning to identify patterns and features from the input data through multiple layers, ultimately producing an output like a category or label.1. Training a Convolutional Neural Network on TAPThe Tanzu Application Platform is a powerful platform that simplifies the development, deployment, and management of modern applications. By combining TAP with Greenplum, an open-source, massively parallel data warehouse, we can efficiently train a Convolutional Neural Network on large-scale data sets. TAP provides the necessary infrastructure and tooling to enable seamless scaling and management of resources, ensuring optimal performance and efficiency throughout the training process.              Tanzu Application Platform &amp;amp; Greenplum2. Discover Data Sets with DataHubData is the building block of any ML project, and having a comprehensive data catalog is essential for discovering and managing data sets. DataHub, a popular data catalog tool, allows users to easily discover, understand, and use data sets across the organization. By integrating DataHub with TAP and Greenplum, we can quickly locate the most relevant data sets for our ML projects and ensure that our data is accurate, consistent, and up-to-date.                    Datahub - The #1 Open Source Data Catalog‚Üí Install Datahub Accelerator for TAPtanzu acc create datahub --git-repository https://github.com/agapebondservant/datahub-accelerator.git --git-branch main3. Set Up a Development Environment with JupyterHub notebooks      Jupyter Notebooks is an open-source web application that allows users to create and share documents containing live code, equations, visualizations, and narrative text. It is widely used for data cleaning, transformation, and exploration, as well as for building and training ML models. By setting up a Jupyter Notebook environment on TAP, we can access our data stored in Greenplum and perform experiments with the latest ML frameworks and libraries, all within a single, unified platform.        JupyterHub is a popular tool for Data Scientist. It is used for hosting Jupyter notebooks. A Jupyter notebook provides a browser-based IDE that enables live coding, experimentation, data exploration and model engineering. JupyterHub is a containerized, open-source app, making it easy to deploy on TAP.                      JupyterLab - A Next-Generation Notebook Interface‚Üí Install Jupyter Utilities Accelerator for TAPtanzu acc create jupyter --git-repository https://github.com/agapebondservant/jupyter-accelerator.git --git-branch main4. Build the ML Model Workflow with MLflowMLflow is an open-source platform that streamlines the end-to-end management of machine learning projects. It provides a unified interface to manage the entire lifecycle of ML models, including experimentation, reproducibility, deployment, and monitoring. By integrating MLflow with TAP and Greenplum, we can easily track and compare experiments, package and share models, and deploy them in a scalable and reproducible manner. This integration ensures a smooth and efficient ML workflow, improving the overall effectiveness of our ML operations.                    mlflow - An open source platform for the machine learning lifecycle‚Üí Install Kubeflow Accelerator for TAPtanzu acc create kubeflowpipelines --git-repository https://github.com/agapebondservant/kubeflow-pipelines-accelerator --git-branch main5. Train scalable Machine Learning models on Greenplum platform using GreenplumPythonGreenplum has strong analytical capabilities that make them well suited for data science problems at a massive scale. Combining its MPP capabilities with Python‚Äôs rich ecosystem makes the end-to-end Machine Learning model development experience significantly faster.To simplify the path to production and operational usage of trained ML models, we can unleash the power of GreenplumPython, it‚Äôs a Python package that enables in-database execution of Python code within Greenplum functions.Data Scientists can then perform complex data processing and analysis tasks using familiar Python syntax and libraries, directly inside the Greenplum database. This integration reduces data movement and improves performance, as data processing occurs close to where the data is stored, making it an efficient way to perform advanced analytics, pre-processing, feature engineering, model training and deployment for your ML projects.‚Üí Greenplum Documentation‚Üí GreenplumPython Package            GreenplumPython for End-To-End MLOps tasks with MLflow6. Build and Train ML Model Workflow with TensorFlow.TensorFlow is a popular open-source ML library developed by Google. It provides a flexible and efficient platform for building and deploying ML models across various platforms and devices. By integrating TensorFlow with TAP and Greenplum, we can develop and train our ML models on massive data sets, harnessing the full power of distributed computing for faster and more accurate results.‚Üí Install Tensorflow Accelerator for TAPtanzu acc create sample-cnn-app --git-repository https://github.com/tanzumlai/sample-ml-app.git --git-branch main7. Build ML Pipeline with Argo WorkflowsArgo Workflows is an open-source, container-native workflow engine for orchestrating parallel jobs on Kubernetes. By integrating Argo Workflows with TAP, we can build and manage complex ML pipelines with ease, automating tasks such as data preprocessing, model training, and deployment. This enables faster experimentation and iteration, ultimately accelerating the delivery of high-quality ML models.                    ArgoCD Workflows - open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.‚Üí Install ArgoCD Workflows Accelerator for TAPtanzu acc create argo-pipelines-acc --git-repository https://github.com/agapebondservant/argo-workflows-accelerator.git --git-branch main‚Üí More info on ArgoCD Workflows8. Create Predictive Apps with APIsOnce our ML models are trained and optimized, we can use TAP to build predictive applications that leverage these models to provide valuable insights and predictions. By exposing our models through APIs, we enable seamless integration with existing applications and systems, ensuring that our data-driven insights can be easily consumed by end-users and decision-makers. This not only increases the value and impact of our ML efforts but also promotes a data-driven culture within the organization.References Architecture        ‚Üí Source code for build a DATA E2E Demo PlatformThese insightfulls References Architecture are credited to Omotola Awofolu - Senior Platform Architect and Caio Farias - Account Data Engineer from VMware.ConclusionIn this blog post, We have demonstrated how combining the Tanzu Application Platform with Greenplum can deliver a full data platform with MLOps capabilities.From training a Convolutional Neural Network on TAP to building predictive apps with APIs, this powerful combination enables organizations to harness the power of their data and accelerate the delivery of high-quality ML models.By integrating tools like DataHub, Jupyter Notebook, Kubeflow, TensorFlow, GreenplumPython and Argo Workflows, we can streamline the entire ML lifecycle, improving efficiency and scalability across the board.With TAP and Greenplum at the core of your data platform, your organization will be well-equipped to tackle the most challenging ML problems and drive innovation in the ever-evolving world of data and AI.        AuthorsThis blog post was co-written with my friend Ahmed Rachid Hazourli, a very bright Tanzu Data Engineer.Special Thanks to Omotola Awofolu - Senior Platform Architect and Caio Farias - Account Data Engineer for their invaluable contribution in developing the TAP accelerator, the Reference Architecture and for being a constant source of inspiration to us.We sincerely hope you enjoyed reading it!Linkedin  Omotola Awofolu  Caio Farias  Ahmed Rachid Hazourli  Fr√©d√©ric Klein",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-08T18:01:35+02:00'>08 May 2023</time><a class='article__image' href='/post/2023/05/08/tap-and-greenplum.html'> <img src='/images/mlops2.png' alt='Discover the power of MLOps with Tanzu Application Platform (TAP) and Greenplum.'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/08/tap-and-greenplum.html'>Discover the power of MLOps with Tanzu Application Platform (TAP) and Greenplum.</a> </h2><p class='article__excerpt'>Learn how to train a Convolutional Neural Network, utilize DataHub for data set management, set up a JupyterHub notebooks environment, and build ML workflows with Kubeflow and Argo Workflows.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Enhancing Developer Experience on Tanzu Application Platform with Backstage",
      "category" : "",
      "tags"     : "Tap",
      "url"      : "/post/2023/05/06/enhancing-devx-on-tap-with-backstage.html",
      "date"     : "May 6, 2023",
      "content"  : "Tanzu Application Platform (TAP)I am consistently impressed by the way the Tanzu Application Platform (TAP) empowers developers to build and manage modern applications seamlessly. One of the key aspects that set TAP apart is its focus on providing an exceptional developer experience. This focus is further strengthened by the integration of the Backstage open-source project, which allows developers to efficiently manage and maintain their applications, ultimately speeding up software delivery. In this blog post, we‚Äôll dive into the benefits of Backstage integration in TAP and explore how it elevates the developer experience.‚Üí Tanzu Application PlatformWhat is Backstage?Backstage is an open-source platform developed by Spotify, which aims to provide a unified developer portal that simplifies the process of managing software components, services, and tools. It offers a single, extensible interface for developers to discover, explore, and interact with their organization‚Äôs software ecosystem, making it easier to navigate the complexities of modern software development.‚Üí Backstage.ioBackstage Integration in Tanzu Application PlatformBackstage is the UI of TAP, and provides a centralized location to manage  applications and services, regardless of the underlying infrastructure. This brings the following benefits to developer:1. Unified Developer Experience: With Backstage, developers can access all their services and applications from a single, intuitive interface. This streamlined experience simplifies the management of application components, reducing the time spent navigating between different tools and platforms.                                          TAP User Interface2. Enhanced Service Catalog: TAP‚Äôs integration with Backstage enables developers to easily discover and access their organization‚Äôs software components, services, and APIs. The catalog provides rich metadata and documentation, allowing teams to quickly understand and start using these resources.                              TAP User Interface and TAP API Portal3. Accelerated Development (aka Accelerator): By providing a comprehensive and consistent interface for managing applications, Backstage helps developers to quickly understand and navigate their organization‚Äôs software ecosystem. This enables them to spend less time on administrative tasks and more time on developing new features and functionality.              TAP User Interface and TAP plugins for Visual Studio Code4. Open-Source Community: Being an open-source project, Backstage boasts a vibrant and active community that continuously contributes to its development. This ensures that TAP users can leverage the latest features and improvements, keeping their developer experience up to date and cutting-edge.ConclusionThe integration of the Backstage open-source project into Tanzu Application Platform plays a pivotal role in enhancing the developer experience. By streamlining application management, promoting discoverability, and providing customization options,‚Üí Tanzu Application Platform empowers developers to focus on what matters most: building software.I am thrilled to see the continuous evolution of TAP and look forward to seeing how Backstage integration will continue to elevate the platform in the future, with customizable plugins.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-06T17:01:35+02:00'>06 May 2023</time><a class='article__image' href='/post/2023/05/06/enhancing-devx-on-tap-with-backstage.html'> <img src='/images/backstage-tap.png' alt='Enhancing Developer Experience on Tanzu Application Platform with Backstage'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/06/enhancing-devx-on-tap-with-backstage.html'>Enhancing Developer Experience on Tanzu Application Platform with Backstage</a> </h2><p class='article__excerpt'>Discover how the Tanzu Application Platform enhances developer experience through the integration of the Backstage open-source project, streamlining application management and boosting productivity..</p></div></div></div>"
    } ,
  
    {
      "title"    : "Tanzu Experience Days",
      "category" : "",
      "tags"     : "tanzu-experience-day",
      "url"      : "/post/2023/05/04/experience-days.html",
      "date"     : "May 4, 2023",
      "content"  : "Join me for VMware‚Äôs free ‚ÄúExperience Days‚Äù sessions dedicated to Tanzu solutions!Are you a technical professional interested in learning how to manage and operate a container-based infrastructure based on Kubernetes? Do you want to discover best practices for improving the development and deployment experience of your applications while strengthening security? If so, join me for VMware‚Äôs free ‚ÄúExperience Days‚Äù sessions dedicated to Tanzu solutions!During these sessions, you‚Äôll participate in in-depth workshops and hands-on exercises specifically designed for technical profiles. My colleagues and I, who are experienced Solutions Engineers, will be there to answer all your technical questions live.The May 16th sessions are already available, so sign up now by clicking on the links below. You can also register for the upcoming sessions on June 13th and July 4th.Here are the links to register for the sessions:      Kubernetes exploitation-oriented session        Developer-oriented session  Register today!Don‚Äôt miss out on this valuable opportunity to deepen your knowledge of Tanzu solutions and enhance your technical skills.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-04T14:01:35+02:00'>04 May 2023</time><a class='article__image' href='/post/2023/05/04/experience-days.html'> <img src='/images/exp.png' alt='Tanzu Experience Days'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/04/experience-days.html'>Tanzu Experience Days</a> </h2><p class='article__excerpt'>Join me for VMware&#39;s free &quot;Experience Days&quot; sessions dedicated to Tanzu solutions!</p></div></div></div>"
    } ,
  
    {
      "title"    : "vSphere Pod vs Tanzu Kubernetes Cluster? A Comprehensive Comparison",
      "category" : "",
      "tags"     : "vsphere-with-tanzu",
      "url"      : "/post/2023/05/03/vsphere-pod-vs-tkc.html",
      "date"     : "May 3, 2023",
      "content"  : "IntroductionIn today‚Äôs fast-paced world of technological advancements, the adoption of containerization and cloud-native application development has skyrocketed. VMware Tanzu offers two powerful tools for managing and deploying containerized applications: vSphere Pod and Tanzu Kubernetes Cluster. In this blog post, we will delve into the differences between these two technologies, discussing their features, advantages, and appropriate use cases, while incorporating insights from VMware‚Äôs official documentation.vSphere Pod: An OverviewvSphere Pod is part of vSphere-with-Tanzu. vSphere-with-Tanzu is a container runtime environment designed for VMware vSphere. It enables you to run and manage containerized applications directly on the vSphere platform. vSphere Pod is the equivalent of a Kubernetes pod that can run directly on the ESXi hypervisor, and provides a lightweight, scalable solution that leverages the power of VMware‚Äôs hypervisor to deliver security, performance, and resource management capabilities.Key Features of vSphere Pod:  Native integration with vSphere: vSphere Pod is integrated into the vSphere platform, making it easy to manage container workloads alongside traditional VM workloads.  VM-level isolation: Each pod runs in its own virtual machine, providing strong isolation between workloads and ensuring that a single compromised pod cannot impact others.  Resource management: vSphere Pod leverages vSphere‚Äôs resource management capabilities, allowing you to allocate resources such as CPU, memory, and storage to your containers.  Namespace-based management: vSphere Pod introduces the concept of namespaces to vSphere, which simplifies the management of container workloads and allows for easy delegation of authority to developers.Tanzu Kubernetes Cluster: An OverviewTanzu Kubernetes Cluster (TKC) is a Kubernetes-based platform for running and managing containerized applications. It is a part of VMware‚Äôs Tanzu portfolio and vSphere-with-Tanzu, which offers a comprehensive set of tools for building, running, and managing modern applications. TKC enables you to deploy and manage Kubernetes clusters on top of vSphere, giving you a consistent, enterprise-grade Kubernetes experience.Key Features of Tanzu Kubernetes Cluster:  Enterprise-grade Kubernetes: TKC provides a fully conformant, upstream-aligned Kubernetes distribution, ensuring compatibility with the broader Kubernetes ecosystem.  Consistent management experience: TKC integrates with VMware Tanzu Mission Control, enabling you to manage Kubernetes clusters across multiple environments and platforms.  Extensibility and customization: With TKC, you can leverage a wide range of Kubernetes add-ons and extensions to tailor your clusters to your specific needs.  Dynamic scaling: TKC supports dynamic scaling of Kubernetes clusters, allowing you to easily adjust the size of your clusters based on your requirements.Comparing vSphere Pod and Tanzu Kubernetes ClusterNow that we have a basic understanding of both vSphere Pod and Tanzu Kubernetes Cluster, let‚Äôs compare them in various aspects:  Complexity: vSphere Pod is a simpler solution compared to TKC, as it is integrated directly into the vSphere platform. TKC, on the other hand, offers a more comprehensive Kubernetes experience but requires additional management and configuration.  Use cases: vSphere Pod is ideal for organizations looking to run containerized applications on vSphere without adopting a full Kubernetes stack. Tanzu Kubernetes Cluster is better suited for organizations that require a complete, enterprise-grade Kubernetes solution and are willing to invest in the necessary infrastructure and management tools.  Ecosystem: TKC offers broader compatibility with the Kubernetes ecosystem, including support for third-party add-ons and extensions. vSphere Pod, being a native vSphere solution, is more limited in this regard.ConclusionThe choice between vSphere Pod and Tanzu Kubernetes Cluster depends on your specific needs, existing infrastructure, and long-term goals. vSphere Pod provides a lightweight, integrated container runtime for vSphere users, while Tanzu Kubernetes Cluster offers a complete, enterprise-grade Kubernetes experience. By understanding the differences between these two technologies, you can make an informed decision that best aligns with your organization‚Äôs requirements and objectives.More info      When to Use vSphere Pods and Tanzu Kubernetes Clusters - VMware Documentations        What Is a vSphere Pod? - VMware Documentations        What Is a Tanzu Kubernetes Cluster? - VMware Documentations  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-03T17:01:35+02:00'>03 May 2023</time><a class='article__image' href='/post/2023/05/03/vsphere-pod-vs-tkc.html'> <img src='/images/vsphere-with-tanzu-vspherepod.png' alt='vSphere Pod vs Tanzu Kubernetes Cluster? A Comprehensive Comparison'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/03/vsphere-pod-vs-tkc.html'>vSphere Pod vs Tanzu Kubernetes Cluster? A Comprehensive Comparison</a> </h2><p class='article__excerpt'>in this blog post, we will delve into the differences between these two technologies, discussing their features, advantages, and appropriate use case.</p></div></div></div>"
    } ,
  
    {
      "title"    : "vSphere-with-Tanzu or k8s in vSphere?",
      "category" : "",
      "tags"     : "vsphere-with-tanzu",
      "url"      : "/post/2023/05/03/vsphere-with-tanzu.html",
      "date"     : "May 3, 2023",
      "content"  : "VMware vSphere-with-Tanzu is a powerful platform that enables businesses to deploy and manage Kubernetes clusters directly on their vSphere infrastructure. Tanzu is a VMware solution designed to simplify the deployment and management of Kubernetes for enterprise organizations. In this blog post, we‚Äôll take a closer look at VMware vSphere with Tanzu and how to use it with Tanzu CLI.What is Tanzu CLI?Tanzu CLI is a command-line tool that provides a simplified interface for managing Kubernetes clusters and applications. It is designed to make it easy to manage Kubernetes clusters across multiple cloud providers, including vSphere with Tanzu.Using Tanzu CLI, you can deploy and manage Kubernetes clusters, deploy and manage applications, and automate common tasks. It is a powerful tool that can help simplify the management of Kubernetes for enterprise organizations.How to use vSphere-with-Tanzu and the Tanzu CLI ?Here‚Äôs a step-by-step guide on how to use VMware vSphere with Tanzu with Tanzu CLI:Step 1: Install the Tanzu CLI and the Kubectl + vSphere pluginThe first step is to install the Tanzu CLI on your local machine. You can do this by asking your vsphere administrator to Get the link to the Kubernetes Command Line Interface Tools download page.Step 2: Configure the vSphere environmentNext, you‚Äôll need to configure your vSphere environment to work with Tanzu CLI. This involves setting up the appropriate credentials and configuring the vSphere endpoint. You can do this by running the following command:kubectl vsphere login --server &amp;lt;vCenter server address&amp;gt; --vsphere-username &amp;lt;vCenter username&amp;gt; --insecure-skip-tls-verifyThis will authenticate you with vSphere and allow you to manage your Kubernetes clusters using Tanzu CLI.Step 3: Create a Tanzu Kubernetes clusterOnce you‚Äôve configured your vSphere environment, you can create a Tanzu Kubernetes cluster using the classical kubectl apply:  First craft the yaml that will describe the cluster topology:apiVersion: run.tanzu.vmware.com/v1alpha2           #TKGS API endpointkind: TanzuKubernetesCluster                        #required parametermetadata:  name: tkgs-dev01                                  #cluster name, user defined  namespace: vns-dev-01spec:  topology:    controlPlane:      replicas: 1                                   #number of control plane nodes      vmClass: best-effort-small                    #vmclass for control plane nodes      storageClass: kubernetes-sp      tkr:          reference:          name: v1.21.6---vmware.1-tkg.1.b3d708a    nodePools:    - name: worker-nodepool-a1      replicas: 1                                   #number of worker nodes      vmClass: best-effort-small                    #vmclass for worker nodes      storageClass: kubernetes-sp      tkr:          reference:          name: v1.21.6---vmware.1-tkg.1.b3d708a  And then apply the file with the kubectl commandkubectl apply -f workload_prod_cluster01.yamlStep 4: Deploy your workloadsWith your Tanzu Kubernetes cluster up and running, you can now deploy your container workloads using Kubernetes manifests or Helm charts. Tanzu CLI provides a number of built-in tools and integrations to make this process as simple and streamlined as possible.  Connect to the Tanzu Kubernetes clusterkubectl vsphere login --server &amp;lt;vCenter server address&amp;gt; --vsphere-username &amp;lt;vCenter username&amp;gt; --insecure-skip-tls-verify -tanzu-kubernetes-cluster-name &amp;lt;tkgs-dev01&amp;gt; --tanzu-kubernetes-cluster-namespace &amp;lt;vns-dev-01&amp;gt;  Apply cluster rolebindingkubectl create clusterrolebinding default-tkg-admin-privileged-binding --clusterrole=psp:vmware-system-privileged --group=system:authenticated  Deploy my blog Appkubectl create deployment fredblog --image=registry.fklein.me/tanzu-blog/fklein-blog:2023-05-04-11-44-18 --port=80  Expose the app (through a Loadbalancer L4 for example)kubectl expose deployment fredblog --port 80 --type LoadBalancer --target-port=80  Check the ip and connect with your browserkubectl get svc Step 5: Manage your Kubernetes clustersUsing Tanzu CLI, you can manage your Kubernetes clusters, including scaling them up or down, updating the Kubernetes version, and more. This provides a powerful command-line interface for managing both your VMs and containers.ConclusionVMware vSphere with Tanzu is a powerful platform that enables businesses to deploy and manage Kubernetes clusters directly on their vSphere infrastructure. Tanzu CLI is a command-line tool that provides a simplified interface for managing Kubernetes clusters and applications. By following the steps outlined above, you can get started with VMware vSphere with Tanzu and Tanzu CLI and begin deploying and managing your container workloads more easily and efficiently.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-03T16:01:35+02:00'>03 May 2023</time><a class='article__image' href='/post/2023/05/03/vsphere-with-tanzu.html'> <img src='/images/vsphere-with-tanzu.png' alt='vSphere-with-Tanzu or k8s in vSphere?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/03/vsphere-with-tanzu.html'>vSphere-with-Tanzu or k8s in vSphere?</a> </h2><p class='article__excerpt'>vSphere-with-Tanzu (aka TKGs) is a powerful platform that enables businesses to deploy and manage Kubernetes clusters directly on their vSphere infrastructure. In this blog post, we&#39;ll take a closer look at VMware vSphere with Tanzu and how to use it with Tanzu CLI.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Tanzu Build Service - Automating Container Image Building and Management",
      "category" : "",
      "tags"     : "tanzu-build-service",
      "url"      : "/post/2023/05/03/tanzu-build-service.html",
      "date"     : "May 3, 2023",
      "content"  : "Tanzu Build Service: Automating Container Image Building and ManagementAs organizations continue to adopt modern software development practices, containers have become a popular way to package and deploy applications. However, building and managing container images can be a time-consuming and error-prone task, especially at scale. Tanzu Build Service is a tool that helps automate this process, making it easier for developers to focus on writing code and delivering business value.What is Tanzu Build Service?Tanzu Build Service is a Kubernetes-native tool that automates the process of building, managing, and updating container images. It leverages Cloud Native Buildpacks, an open-source technology that provides a modular and composable approach to building containers. Cloud Native Buildpacks automatically detect the language and framework of an application and create optimized container images that are free from security vulnerabilities.Tanzu Build Service provides a declarative way to define how container images should be built, including which base images to use, which buildpacks to include, and how to configure the resulting images. This declarative approach ensures that all images are built consistently, regardless of the developer or environment.How does Tanzu Build Service work?Tanzu Build Service uses a set of controllers and operators to manage the entire container image build process. Developers define a build configuration, which specifies the source code location, buildpacks, and other build parameters. Tanzu Build Service then creates a build plan that determines the exact build steps required to create the container image. Finally, the build process is executed using Cloud Native Buildpacks, which create a secure and optimized container image.Once the container image is built, Tanzu Build Service automatically stores it in a container registry, such as Harbor or Docker Hub. This ensures that the image is available to all developers, regardless of their location or access permissions.  With one line, you can create an OCI that is listening to a git repository, and then push the image to a registrykp image create fkleinblog --tag registry.fklein.me/tbs/fkleinblog:1.1 --git https://github.com/fklein82/tbs-blog-demo.git --git-revision main -n dev  You can check the buiding log of the imagekp build logs fkleinblog -n dev  And check the image listkp image list -n devWhat are the benefits of Tanzu Build Service?Tanzu Build Service provides several benefits for developers and IT organizations, including:Consistency: Tanzu Build Service ensures that all container images are built using the same base images, buildpacks, and configurations, regardless of the developer or environment.Security: Cloud Native Buildpacks automatically detect and remediate security vulnerabilities, ensuring that all container images are free from known security issues.Speed: Tanzu Build Service automates the build process, reducing the time required to build and update container images.Scalability: Tanzu Build Service can be easily scaled to support large and complex applications, making it an ideal solution for enterprise environments.Ease of use: Tanzu Build Service provides a simple and intuitive interface for developers to define and manage container images, reducing the need for manual intervention.ConclusionTanzu Build Service is a powerful tool that automates the process of building, managing, and updating container images. By leveraging Cloud Native Buildpacks, Tanzu Build Service provides a secure, consistent, and scalable approach to container image management, freeing developers to focus on writing code and delivering business value. If you‚Äôre looking to streamline your container image management process, Tanzu Build Service is definitely worth considering.More information      Source code Application demo to test TBS        Script to Make a demo with TBS on a Kubernetes CNFC Compliant  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-05-03T14:01:35+02:00'>03 May 2023</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2023/05/03/tanzu-build-service.html'> <img src='/images/06.png' alt='Tanzu Build Service - Automating Container Image Building and Management'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/05/03/tanzu-build-service.html'>Tanzu Build Service - Automating Container Image Building and Management</a> </h2><p class='article__excerpt'>Tanzu Build Service is a Kubernetes-native tool that automates the process of building, managing, and updating container images. How do I use TBS?</p></div></div></div>"
    } ,
  
    {
      "title"    : "VMware Application Catalog - A Curated image catalog",
      "category" : "",
      "tags"     : "vac and vmware-application-catalog",
      "url"      : "/post/2023/04/06/vmware-application-catalog-demo.html",
      "date"     : "Apr 6, 2023",
      "content"  : "The VMware Application Catalog is a central repository of Open-Source software that is available for use with Virtualized and Kubernetes Infrastructure. It contains a vast array of pre-packaged software applications and services, including operating systems, middleware, and databases, that are securized to work with virtualization and cloud computing platform.The VMware Application Catalog provides a single, consolidated source for organizations to discover, deploy, and manage software applications, simplifying the process of identifying and selecting software solutions for use within their ecosystem. It allows IT teams to quickly search for and identify the applications they need, and then deploy them to their  infrastructure with confidence, knowing that they have been pre-tested and verified to work with VMware‚Äôs products.The catalog is continually updated and expanded, with new applications and services being added on an ongoing basis. It also includes a range of community-contributed packages, providing access to a wide range of open-source software solutions that have been customized and optimized for use with VMware‚Äôs products or Cloud Public Platform.Overall, the VMware Application Catalog provides significant value to organizations by simplifying the process of identifying, selecting, and deploying software applications within their virtualized infrastructure. By providing a central repository of pre-tested and verified software solutions, it enables IT teams to streamline their software deployment processes, reduce risk, and increase the efficiency and effectiveness of their virtualized infrastructure.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-04-06T14:01:35+02:00'>06 Apr 2023</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2023/04/06/vmware-application-catalog-demo.html'> <img src='/images/05.png' alt='VMware Application Catalog - A Curated image catalog'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/04/06/vmware-application-catalog-demo.html'>VMware Application Catalog - A Curated image catalog</a> </h2><p class='article__excerpt'>Trusted, pre-packaged application components that are continuously maintained and verifiably tested for use in production environments. How do I use VMware Application Catalog?</p></div></div></div>"
    } ,
  
    {
      "title"    : "Tanzu Mission Control - Streamlining Kubernetes Management Across Multiple Clusters",
      "category" : "",
      "tags"     : "tanzu-mission-control",
      "url"      : "/post/2023/03/02/tanzu-mission-constrol.html",
      "date"     : "Mar 2, 2023",
      "content"  : "Tanzu Mission Control: Streamlining Kubernetes Management Across Multiple ClustersAs organizations continue to adopt Kubernetes as their container orchestration platform of choice, managing multiple clusters can become a complex and time-consuming task. Tanzu Mission Control is a tool that helps streamline Kubernetes management across multiple clusters, providing a centralized interface for administrators to monitor and manage their Kubernetes environments.What is Tanzu Mission Control?Tanzu Mission Control is a Kubernetes management platform that provides a centralized interface for administrators to manage and monitor multiple Kubernetes clusters across different environments, such as on-premises, public cloud, or hybrid. Tanzu Mission Control allows administrators to create and enforce policies, manage access control, and monitor Kubernetes usage and performance across all clusters.Tanzu Mission Control is built on top of VMware Tanzu, a portfolio of products and services that help organizations build, run, and manage modern applications on Kubernetes.How does Tanzu Mission Control work?Tanzu Mission Control provides a centralized management plane for administrators to manage multiple Kubernetes clusters. Administrators can connect multiple clusters to Tanzu Mission Control using either a self-hosted cluster agent or a cloud-based cluster agent.Once the clusters are connected, administrators can manage them through a unified interface. Tanzu Mission Control provides a set of tools to manage Kubernetes clusters, including:Cluster Lifecycle Management: Administrators can create, update, and delete Kubernetes clusters from a single interface.Policy Management: Administrators can create and enforce policies across all clusters, such as network policies, pod security policies, and resource quotas.Access Control: Administrators can manage user access to Kubernetes resources across all clusters, including role-based access control (RBAC) and Kubernetes namespaces.Compliance Management: Tanzu Mission Control provides built-in compliance reports and alerts, allowing administrators to monitor Kubernetes usage and ensure compliance with internal and external policies.Application Management: Tanzu Mission Control provides tools to manage applications across multiple clusters, including deployment, scaling, and monitoring.What are the benefits of Tanzu Mission Control?Tanzu Mission Control provides several benefits for organizations that manage multiple Kubernetes clusters, including:Centralized Management: Tanzu Mission Control provides a single interface to manage multiple Kubernetes clusters across different environments, reducing the need for manual intervention and improving efficiency.Consistency: Tanzu Mission Control allows administrators to enforce consistent policies and access control across all clusters, ensuring a consistent experience for developers and end-users.Visibility: Tanzu Mission Control provides detailed insights into Kubernetes usage and performance across all clusters, allowing administrators to identify issues and optimize resource usage.Compliance: Tanzu Mission Control provides built-in compliance reports and alerts, helping organizations maintain compliance with internal and external policies.Scalability: Tanzu Mission Control is designed to scale to support large and complex Kubernetes environments, making it an ideal solution for enterprise organizations.ConclusionTanzu Mission Control is a powerful tool that streamlines Kubernetes management across multiple clusters. By providing a centralized interface for administrators to manage and monitor Kubernetes environments, Tanzu Mission Control reduces the complexity of managing Kubernetes at scale. If you‚Äôre looking to simplify your Kubernetes management processes and improve efficiency, Tanzu Mission Control is definitely worth considering.Click Here to test the product !",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-03-02T07:01:35+01:00'>02 Mar 2023</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2023/03/02/tanzu-mission-constrol.html'> <img src='/images/07.png' alt='Tanzu Mission Control - Streamlining Kubernetes Management Across Multiple Clusters'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2023/03/02/tanzu-mission-constrol.html'>Tanzu Mission Control - Streamlining Kubernetes Management Across Multiple Clusters</a> </h2><p class='article__excerpt'>Tanzu Mission Control is a Kubernetes management platform that provides a centralized interface for administrators to manage and monitor multiple Kubernetes clusters across different environments, such as on-premises, public cloud, or hybrid.</p></div></div></div>"
    } ,
  
    {
      "title"    : "VMware Tanzu Application Platform (TAP) - French demo",
      "category" : "",
      "tags"     : "tap",
      "url"      : "/post/2022/08/09/vmware-tanzu-multi-cloud-paas-demo.html",
      "date"     : "Aug 9, 2022",
      "content"  : "The Tanzu Application Platform (TAP) is a cloud-native platform for building, deploying, and managing modern applications. It provides developers with a comprehensive set of tools and services to streamline the application development process and accelerate time to market. Here are some of the key ways that TAP delivers value to organizations:      Faster application delivery: TAP provides a cloud-native platform for building and deploying applications, with support for multiple programming languages and frameworks. This enables developers to build applications quickly and easily, and deploy them to production faster than with traditional methods.        Increased developer productivity: TAP provides a set of developer-centric tools and services, such as integrated development environments (IDEs) and code repositories, that streamline the application development process. This increases developer productivity and enables them to focus on building applications, rather than managing infrastructure.        Consistent, secure environment: TAP provides a consistent, secure environment for running applications across multiple clouds and environments. This reduces the risk of application downtime or security breaches caused by differences in underlying infrastructure.        Simplified application management: TAP provides a centralized management platform for applications, enabling administrators to monitor and manage applications across multiple clouds and environments. This simplifies the management of complex application environments and reduces the risk of application downtime.        Scalable infrastructure: TAP provides a scalable infrastructure that can easily accommodate the needs of growing applications. It enables organizations to scale infrastructure up or down as needed, without having to worry about the underlying infrastructure.  Overall, the Tanzu Application Platform delivers significant value to organizations by providing a cloud-native platform for building, deploying, and managing modern applications. By accelerating application delivery, increasing developer productivity, providing a consistent and secure environment, simplifying application management, and providing a scalable infrastructure, TAP enables organizations to build and deploy applications faster, with higher quality and at lower cost.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-09T14:01:35+02:00'>09 Aug 2022</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2022/08/09/vmware-tanzu-multi-cloud-paas-demo.html'> <img src='/images/04.png' alt='VMware Tanzu Application Platform (TAP) - French demo'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2022/08/09/vmware-tanzu-multi-cloud-paas-demo.html'>VMware Tanzu Application Platform (TAP) - French demo</a> </h2><p class='article__excerpt'>TAP is a superior multi-cloud developer experience on Kubernetes. VMware Tanzu Application Platform is a modular, application-aware platform that provides a rich set of developer tooling and a a prepaved path to production, to build and deploy software quickly and securely on any compliant public cloud or on-premises Kubernetes cluster.</p></div></div></div>"
    } ,
  
    {
      "title"    : "How to accelerate the Kubernetes adoption for developers?",
      "category" : "",
      "tags"     : "path-to-prod",
      "url"      : "/post/2022/08/09/accelerate-the-kubernetes-adoption-for-developers.html",
      "date"     : "Aug 9, 2022",
      "content"  : "The VMware Tanzu portfolio is designed to help developers accelerate their adoption of Kubernetes by providing a range of tools and services that simplify the deployment and management of Kubernetes-based applications. Here are some of the key ways that Tanzu products can help developers accelerate their Kubernetes adoption:Simplified Kubernetes deployment: Tanzu Kubernetes Grid (TKG) is a Kubernetes runtime that provides a consistent and secure Kubernetes environment across multiple clouds and environments. TKG simplifies the deployment of Kubernetes, allowing developers to quickly spin up Kubernetes clusters and start deploying applications.Streamlined application deployment: Tanzu Application Platform is a platform for deploying and managing modern applications. TAP provides a cloud-native runtime for applications, with support for multiple programming languages and frameworks. TAP streamlines the deployment of applications to Kubernetes clusters, reducing the time and effort required to get applications up and running.Centralized management: Tanzu Mission Control (TMC) is a central management platform for Kubernetes clusters and applications running on them. TMC provides a single pane of glass for managing Kubernetes clusters across multiple clouds, and enables policy-based management of applications and infrastructure. TMC simplifies the management of Kubernetes environments, allowing developers to focus on building and deploying applications.Improved visibility and monitoring: Tanzu Observability (TO) is a comprehensive monitoring and analytics platform for modern applications. TO provides real-time visibility into application performance, with built-in dashboards and alerts for identifying and resolving issues quickly. This enables developers to proactively monitor their applications and address issues before they become problems.Automated container image builds: Tanzu Build Service (TBS) is a platform for automating the build and deployment of container images. TBS enables developers to easily create secure and compliant container images from source code, and automates the deployment of these images to Kubernetes clusters. This streamlines the container image build and deployment process, freeing up developers to focus on building applications.Overall, the VMware Tanzu portfolio provides a range of tools and services that can help developers accelerate their adoption of Kubernetes. By simplifying Kubernetes deployment and management, streamlining application deployment, providing centralized management, improving visibility and monitoring, and automating container image builds, Tanzu products can help developers focus on building applications, rather than managing infrastructure.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-09T14:01:35+02:00'>09 Aug 2022</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2022/08/09/accelerate-the-kubernetes-adoption-for-developers.html'> <img src='/images/03.png' alt='How to accelerate the Kubernetes adoption for developers?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2022/08/09/accelerate-the-kubernetes-adoption-for-developers.html'>How to accelerate the Kubernetes adoption for developers?</a> </h2><p class='article__excerpt'>The path to production from a developer point of view. E2E Demo in French.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Tanzu Portfolio Overview in French",
      "category" : "",
      "tags"     : "tanzu-portfolio",
      "url"      : "/post/2022/07/14/portfolio-vmware-tanzu.html",
      "date"     : "Jul 14, 2022",
      "content"  : "VMware Tanzu is a portfolio of products and services designed to help organizations build, run, and manage modern applications on any cloud infrastructure. The Tanzu portfolio is built on top of VMware‚Äôs vSphere virtualization technology, which provides a foundation for running traditional workloads, and extends it to support modern container-based applications.The Tanzu portfolio consists of several components that can be used individually or together as a comprehensive solution for application modernization. These components include:      Tanzu Kubernetes Grid: This is a Kubernetes runtime that can be deployed on any infrastructure, including vSphere, public clouds, and edge environments. Tanzu Kubernetes Grid provides a consistent and secure Kubernetes environment across multiple clouds, simplifying operations and enabling application portability.        Tanzu Application Platform: This is a platform for deploying and managing modern applications. Tanzu Application Service provides a cloud-native runtime for applications, with support for multiple programming languages and frameworks, and integration with popular development tools and services.        Tanzu Mission Control: This is a central management platform for Kubernetes clusters and applications running on them. Tanzu Mission Control provides a single pane of glass for managing Kubernetes clusters across multiple clouds, and enables policy-based management of applications and infrastructure.        Tanzu Observability: This is a comprehensive monitoring and analytics platform for modern applications. Tanzu Observability provides real-time visibility into application performance, with built-in dashboards and alerts for identifying and resolving issues quickly.        Tanzu Build Service: This is a platform for automating the build and deployment of container images. Tanzu Build Service enables developers to easily create secure and compliant container images from source code, and automates the deployment of these images to Kubernetes clusters.        Tanzu Service Mesh: This is a platform for managing microservices-based applications. Tanzu Service Mesh provides visibility and control over service-to-service communication, and enables policy-based management of traffic and security.  Overall, the VMware Tanzu portfolio provides a comprehensive solution for modernizing applications and simplifying operations across multiple clouds and environments. Whether you are looking to build new cloud-native applications or modernize existing ones, Tanzu can help you achieve your goals with ease and efficiency.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-14T14:01:35+02:00'>14 Jul 2022</time><div class='video-icon'> <div class='circle pulse'></div><div class='circle'> <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'> <polygon points='40,30 65,50 40,70'></polygon> </svg> </div></div><a class='article__image' href='/post/2022/07/14/portfolio-vmware-tanzu.html'> <img src='/images/02.png' alt='Tanzu Portfolio Overview in French'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2022/07/14/portfolio-vmware-tanzu.html'>Tanzu Portfolio Overview in French</a> </h2><p class='article__excerpt'>VMware Tanzu portfolio, products and business value overview - French version</p></div></div></div>"
    } 
  
]

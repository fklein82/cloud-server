[
  
    {
      "title"    : "Supercharging OpenShift with MCP - Natural Language Meets Cloud-native Ops",
      "category" : "",
      "tags"     : "AI",
      "url"      : "/post/2025/04/14/ocp-claude.html",
      "date"     : "Apr 14, 2025",
      "content"  : "From KubeCon to Openshift: Why I Built This DemoAfter attending the KubeCon Europe 2025, one major takeaway was clear:  AI isn‚Äôt just a trend, it‚Äôs redefining the future of infrastructure and developer toolsMany sessions focused on how to hide infrastructure complexity through approaches like Platform Engineering and AI-powered developer tools. I came home inspired to test it out myself.So I built a simple demo: using Claude Desktop, OpenShift, and the Model Context Protocol (MCP) to show how natural language can drive real actions in a live Kubernetes cluster.And yes ‚Äî it works. Let me show you how.What is MCP ‚Äì Model Context Protocol?Connecting AI models to real-world systems like APIs, databases, or Kubernetes is usually complex and requires custom code.The Model Context Protocol (MCP), created by Anthropic, is an open standard that makes this much easier.It allows AI models (like Claude) to understand the context and safely perform actions by talking to small programs called MCP servers.A Kubernetes MCP Server, Built by Marc NuriMy colleague Marc at Red Hat has developed a open-source MCP server implementation:üîó kubernetes-mcp-serverThis MCP server can:  Connect to any Kubernetes or OpenShift cluster  Read and manage pods, namespaces, events, deployments, and more  Show pod logs, execute commands, and even run containers  Work seamlessly with Claude Desktop or VS CodeIt runs locally with no dependency on kubectl, helm, or other CLI tools ‚Äî lightweight, flexible, and ready to use.Live Demo: Claude Diagnoses and Fixes a Broken Pod on OpenShiftLet‚Äôs walk through a simple and visual demo you can try today.We‚Äôll deploy a pod that fails to start (due to a bad image), then use Claude to identify the issue and fix it ‚Äî using natural language only.What You Need  A Mac with Homebrew  Claude Desktop installed  A running OpenShift cluster (for example on AWS like me)  OpenShift CLI (oc):brew install openshift-cliMake sure you can log in to your OpenShift cluster:oc login &amp;lt;cluster-url&amp;gt; --token=sha256-...üîß Step 1: Connect Claude to the MCP serverOpen this file:open ~/Library/Application\ Support/Claude\ Desktop/claude_desktop_config.jsonAnd add:{  &quot;mcpServers&quot;: {    &quot;kubernetes&quot;: {      &quot;command&quot;: &quot;npx&quot;,      &quot;args&quot;: [        &quot;-y&quot;,        &quot;kubernetes-mcp-server@latest&quot;      ]    }  }}Then restart Claude Desktop and verify that the MCP Server is running.Step 2: Create a Broken PodLet‚Äôs create a pod in a new namespace, using a non-existent image.oc create namespace democat &amp;lt;&amp;lt;EOF | oc apply -f -apiVersion: v1kind: Podmetadata:  name: mon-app  namespace: demo  labels:    app: mon-appspec:  containers:    - name: web      image: nginx:doesnotexist      ports:        - containerPort: 80EOFThe pod will fail with an ImagePullBackOff error.Step 3: Let Claude Fix ItIn Claude Desktop, simply type:Diagnose and fix the pod `mon-app` in the `demo` namespace.Claude will:  Inspect the pod and retrieve logs and events  Detect the image error  Suggest replacing the image with a working version (nginx:latest)  Apply the change if you confirmAll this, using natural language and the MCP server ‚Äî no CLI needed.Step 4: Clean UpOnce you‚Äôre done:oc delete namespace demoSecurity ConsiderationsWhile this is a great demo, keep in mind:  The MCP server uses your ~/.kube/config, meaning full access to your cluster  You should always use dedicated service accounts with limited permissions (RBAC)  Don‚Äôt test this on production environments  Monitor all actions and keep audit logs enabledThe good news: the MCP protocol itself supports permission control and sandboxing ‚Äî but you still need to apply best practices.Bonus: OpenShift Lightspeed ‚ö°Ô∏èRed Hat also offers a built-in AI assistant for OpenShift:üëâ OpenShift LightspeedLightspeed enables:  YAML generation and validation with AI  Built-in troubleshooting suggestions  Secure, production-ready GenAI workflows ‚Äî right in the OpenShift web consoleIf MCP + Claude is the open playground for experimentation, Lightspeed is the enterprise solution, fully integrated and supported.üß≠ What‚Äôs Next?In my next blog post, I‚Äôll dive into OpenShift Lightspeed ‚Äî how it works, and how it helps DevOps teams.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2025-04-14T09:00:00+02:00'>14 Apr 2025</time><a class='article__image' href='/post/2025/04/14/ocp-claude.html'> <img src='/images/MCP-OCP2.png' alt='Supercharging OpenShift with MCP - Natural Language Meets Cloud-native Ops'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2025/04/14/ocp-claude.html'>Supercharging OpenShift with MCP - Natural Language Meets Cloud-native Ops</a> </h2><p class='article__excerpt'>Use natural language to control OpenShift with Claude Desktop and the open-source MCP server.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Hashicorp Terraform Entreprise with Red Hat Ansible Automation Platform",
      "category" : "",
      "tags"     : "IAC",
      "url"      : "/post/2024/12/19/tfe-x-aap.html",
      "date"     : "Dec 19, 2024",
      "content"  : "As IT environments shift from traditional on-premise data centers to multi-cloud ecosystems, organizations need scalable, secure, and efficient solutions for Infrastructure as Code (IaC).¬†By leveraging Red Hat‚Äôs Ansible Automation Platform and HashiCorp‚Äôs Terraform Enterprise, organizations can unlock new levels of automation and governance across hybrid and multi-cloud resources.The Power of IntegrationThe Ansible Automation Platform excels in application deployment and configuration management, while Terraform Enterprise is the go-to solution for infrastructure provisioning at scale.¬†Together, they enable:  Provisioning Infrastructure: Terraform Enterprise allows for the provisioning of scalable and secure IaaS or PaaS resources.  Configuring and Enforcing Compliance: Ansible Automation Platform finalizes infrastructure configurations, applies policies, and enforces them to meet organizational standards.Some Use Cases for AAP and TFE Integration examples :¬†  Dynamic Data and Configuration:          Configuring operating systems or application environments.      Adjusting database configurations, logical files, or connection strings.        Compliance at Launch:          Enforcing organizational policies directly via Ansible playbooks.        Hybrid Environments:          Managing environments with components that require both API-based and SSH-based interactions.      This integration addresses critical challenges such as drift management, state file corruption, and performance bottlenecks in CI/CD workflows.Integration Methods: AAP Provider or Run TasksThe integration between Ansible Automation Platform (AAP) and Terraform Enterprise (TFE) can be achieved using two main methods: the AAP Provider for Terraform or Terraform Run Tasks. Each method offers distinct advantages depending on the use case.  AAP Provider for Terraform: This approach integrates Ansible into the Terraform workflow by leveraging playbooks. It enables the orchestration of infrastructure, dynamic inventory management, and the execution of Ansible jobs as part of a Terraform run. This tightly coupled approach is well-suited for scenarios where provisioning and configuration management need to be closely aligned.  Run Tasks: Terraform Enterprise‚Äôs run tasks allow Ansible Automation Platform workflows to be triggered at specific stages of a Terraform run. This loosely coupled method ensures modularity, enabling Terraform and Ansible to operate independently while maintaining a synchronized workflow. Keep in mind that inventory synchronisation needs to be developed there.By selecting the appropriate method based on operational needs, organizations can maximize the benefits of both platforms, achieving efficient infrastructure automation while ensuring scalability and flexibility.Inventory ManagementA key element in the integration between Ansible Automation Platform (AAP) and Terraform Enterprise (TFE) is inventory management. Ansible Automation Platform relies on inventories to define which systems and resources it manages. When integrated with Terraform Enterprise, inventories can be dynamically updated during the provisioning process. This ensures that newly provisioned resources are immediately included within Ansible‚Äôs scope for configuration and compliance.With dynamic inventory management:  Automatic Registration: Terraform can automatically register newly provisioned infrastructure resources into Ansible‚Äôs inventory.  Adaptive Management: AAP can dynamically adapt to changes in the environment without requiring manual updates, ensuring consistent configuration and management of all resources.  Simplified Complexity: Hybrid or multi-cloud environments benefit from seamless synchronization between Terraform‚Äôs provisioning and Ansible‚Äôs configuration tasks.This dynamic approach significantly reduces the risk of configuration drift and ensures that infrastructure changes are accurately reflected in automation workflows, enhancing both efficiency and reliability.Architectural Overview :The integration follows a GitLab ‚Üí Terraform Enterprise ‚Üí Ansible Automation Platform workflow:  Code Submission: Developers push code to GitLab.  Provisioning: Terraform Enterprise provisions infrastructure resources.  Configuration: Ansible Automation Platform applies the necessary configurations and enforces compliance policies.This approach ensures a consistent, traceable, and automated pipeline from infrastructure creation to configuration.Steps details :¬†AAP Provider example¬† :¬†Prerequisites: we need the playbook id to be able to call it from the AAP Provider (it can be obtained from the AAP UI url). Of course it can also be automated another way to retrieve the id from the playbook name.https://registry.terraform.io/providers/ansible/aap/latest/docs/resources/job¬†provider &quot;aap&quot; { host                 = var.aap_host_url username             = var.aap_username password             = var.aap_password}variable &quot;aap_host_url&quot; { type = string}variable &quot;aap_username&quot; { type = string}variable &quot;aap_password&quot; { type = string}variable &quot;job_template_id&quot; { type        = number description = &quot;The job template id&quot;}resource &quot;aap_inventory&quot; &quot;my_inventory&quot; {  name = &quot;TFE_Apache-servers&quot;}resource &quot;aap_host&quot; &quot;create_host&quot; { inventory_id = aap_inventory.my_inventory.id name         = data.terraform_remote_state.aws-ec2.outputs.ec2_first_addr}resource &quot;aap_job&quot; &quot;run_job_template&quot; { job_template_id = var.job_template_id inventory_id    = aap_inventory.my_inventory.id extra_vars = &amp;lt;&amp;lt;EOT{ &quot;inventory&quot;: &quot;${aap_inventory.my_inventory.name}&quot;}EOT}Terraform AAP Provider lifecycle and inventory impact :¬†Create PhaseWhen Terraform provisions new infrastructure (e.g., VMs, containers, networks), the Terraform Provider for AAP updates the AAP inventory. This means that as soon as new resources are provisioned, the AAP inventory is automatically updated to include these new resources. This dynamic inventory management ensures that Ansible can immediately start managing and configuring the newly provisioned resources.Destroy PhaseWhen Terraform destroys resources, the Terraform Provider for AAP updates the AAP inventory to remove the corresponding entries for the destroyed resources. This ensures that Ansible no longer attempts to manage or interact with resources that no longer exist, maintaining the accuracy and relevance of the inventory.Dynamic Inventory ManagementBy integrating Terraform with the Ansible Automation Platform, you can create a dynamic inventory management system. This system automatically updates the inventory based on the current state of your infrastructure, as managed by Terraform. This is particularly useful in environments where infrastructure changes frequently, as it reduces the need for manual updates to the inventory.Automation WorkflowCombining Terraform and Ansible allows for a seamless automation workflow. Terraform handles the provisioning and de-provisioning of infrastructure, while Ansible takes care of configuration management and application deployment. This integration ensures that your infrastructure and applications are always in sync, reducing the risk of configuration drift and improving overall efficiency.Example Workflow :¬†  Provision Infrastructure: Terraform provisions new resources and updates the AAP inventory.  Update Inventory: Ansible reads the updated AAP inventory and is ready to manage the new resources.  Run Playbooks: Ansible runs playbooks to configure and deploy applications on the newly provisioned resources.  Destroy Infrastructure: Terraform destroys resources and updates the AAP inventory.  Remove Inventory Entries: Ansible removes the corresponding entries from its inventory.This lifecycle ensures that Ansible always has an accurate view of the current state of your infrastructure, enabling efficient and reliable automation.Take Aways :¬†Combining Terraform Enterprise and Ansible Automation Platform offers organizations a powerful, cohesive solution for managing infrastructure in complex, multi-cloud environments. This integration provides scalability, security, and efficiency, enabling teams to accelerate delivery while minimizing risk.Sources :AAP Terraform provider:¬†https://github.com/ansible/terraform-provider-aap¬†(warning : the Terraform AAP Provider for AAP 2.5 is not yet released)Github Demo: https://github.com/Sokren/aap-tfcAuthorsThis blog post was co-written with my friends from HashiCorp R√©mi Salandre, √áetin Ardal, and from Red Hat David JakubowiczLinkedin :  R√©mi Salandre - HashiCorp  √áetin ARDAL - HashiCorp  David Jakubowicz - Red Hat  Fr√©d√©ric Klein - Red Hat            ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-12-19T13:01:35+01:00'>19 Dec 2024</time><a class='article__image' href='/post/2024/12/19/tfe-x-aap.html'> <img src='/images/aap.png' alt='Hashicorp Terraform Entreprise with Red Hat Ansible Automation Platform'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2024/12/19/tfe-x-aap.html'>Hashicorp Terraform Entreprise with Red Hat Ansible Automation Platform</a> </h2><p class='article__excerpt'>Optimize your Infrastructure as Code with Terraform Enterprise and Ansible Automation Platform</p></div></div></div>"
    } ,
  
    {
      "title"    : "Building a Container Image on OpenShift Using GitLab CI/CD",
      "category" : "",
      "tags"     : "Openshift",
      "url"      : "/post/2024/06/11/gitlab-openshift.html",
      "date"     : "Jun 11, 2024",
      "content"  : "Building and Deploying a Container Image on OpenShift using GitLab CI/CDIn this blog post, we‚Äôll walk through the process of building a container image using OpenShift and GitLab CI/CD. We will utilize the .gitlab-ci.yml file, a Dockerfile, and some simple HTML and Python code to demonstrate the build process. This guide assumes you have some basic knowledge of GitLab CI/CD and OpenShift.PrerequisitesBefore we begin, ensure you have the following:  Access to an OpenShift cluster. You can try Openshift with our free Sandbox : developer-sandbox  A GitLab repository. GitLab Ultimate free trial  Necessary permissions to create and manage projects in OpenShift.Step 1: Setting up the GitLab CI/CD PipelineFirst, we need to define our GitLab CI/CD pipeline in a .gitlab-ci.yaml file. This file specifies the stages and jobs that GitLab will execute.Here‚Äôs an example of the .gitlab-ci.yaml file:stages:          # List of stages for jobs, and their order of execution  - buildbuild-job:       # This job runs in the build stage, which runs first.  stage: build  script:    - wget -qO- https://downloads-openshift-console.apps.cluster-h4js2.sandbox553.opentlc.com/amd64/linux/oc.tar  |tar xf -     - chmod +x ./oc     - echo &quot;Login to OCP&quot;    - ./oc login --insecure-skip-tls-verify=true --token=&quot;$OCP_TOKEN&quot; --server=&quot;$OCP_SERVER&quot;    - echo &quot;Selecting project&quot;    - ./oc project &quot;${OCP_PROJECT}&quot;    - ./oc start-build my-docker-build --from-dir . -F# Change : https://downloads-openshift-console.apps.cluster-h4js2.sandbox553.opentlc.com with your Openshift API url.Add the Variables in the project on GITLAB :  OCP_PROJECT : name of the namespace in Openshift for the project  OCP_SERVER  : url of the Openshift API  OCP_TOKEN   : Token for the authentification on OpenshiftStep 2: Creating the Container File (Dockerfile)Next, we need to create a Dockerfile which will define our container image. Here‚Äôs a simple example:FROM python:3ADD index.html index.htmlADD server.py server.pyEXPOSE 8888ENTRYPOINT [&quot;python3&quot;, &quot;server.py&quot;]Step 3: Adding Application CodeWe‚Äôll create a simple HTML file and a Python server script to include in our container image.index.html:&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;div align=&quot;center&quot;&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Test OK&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;img src=&quot;https://blog.fklein.me/images/Logo-Red_Hat.png&quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;server.py:#!/usr/bin/python3from http.server import BaseHTTPRequestHandler, HTTPServerimport timeimport jsonfrom socketserver import ThreadingMixInimport threadinghostName = &quot;0.0.0.0&quot;serverPort = 8888class Handler(BaseHTTPRequestHandler):  def do_GET(self):      # curl http://&amp;lt;ServerIP&amp;gt;/index.html      if self.path == &quot;/&quot;:          # Respond with the file contents.          self.send_response(200)          self.send_header(&quot;Content-type&quot;, &quot;text/html&quot;)          self.end_headers()          content = open(&#39;index.html&#39;, &#39;rb&#39;).read()          self.wfile.write(content)      else:          self.send_response(404)      returnclass ThreadedHTTPServer(ThreadingMixIn, HTTPServer):  &quot;&quot;&quot;Handle requests in a separate thread.&quot;&quot;&quot;if __name__ == &quot;__main__&quot;:  webServer = ThreadedHTTPServer((hostName, serverPort), Handler)  print(&quot;Server started http://%s:%s&quot; % (hostName, serverPort))  try:      webServer.serve_forever()  except KeyboardInterrupt:      pass  webServer.server_close()  print(&quot;Server stopped.&quot;)Step 4: Configuring OpenShiftAs we want to build from a Dockerfile, we will use the dockerstrategy with binary source (which allow to send local files from gitlab runner to OpenShift) :More information hereTo build our Docker image in OpenShift, we need to create a BuildConfig. This configuration can be created using the following command:oc new-build --binary --strategy=docker --name my-docker-buildStep 5: Running the BuildThe final step is to trigger the build from GitLab. The .gitlab-ci.yaml file we created earlier includes the necessary commands to login to OpenShift, select the project, and start the build.You just need to commit the files to the GITLAB repo to trigger the build.Step 6: Deploy the containerOn Openshift UI, on the Developer profile, click on +Add, and Container ImagesSelect Image stream tag from internal registry and select the image you have builded, and click on create.Your app is deployedConclusionWith these steps, you‚Äôve set up a CI/CD pipeline in GitLab to build a Docker image using OpenShift. This integration allows you to leverage the powerful features of both platforms to streamline your development and deployment processes.For more detailed information on builds in OpenShift, you can refer to the Builds for OpenShift Overview.Happy building!References  Red Hat OpenShift Documentation  GitLab CI/CD DocumentationThis guide should help you get started with building and deploying container images using GitLab CI/CD and OpenShift. If you have any questions or run into issues, feel free to reach out for support.AuthorsThis blog post was co-written with my friend Sebastien Lallemand.Linkedin :  Sebastien Lallemand  Fr√©d√©ric Klein             ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-06-11T14:01:35+02:00'>11 Jun 2024</time><a class='article__image' href='/post/2024/06/11/gitlab-openshift.html'> <img src='/images/02.png' alt='Building a Container Image on OpenShift Using GitLab CI/CD'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/post/2024/06/11/gitlab-openshift.html'>Building a Container Image on OpenShift Using GitLab CI/CD</a> </h2><p class='article__excerpt'>Learn how to streamline your CI/CD pipeline by integrating GitLab with OpenShift to build and deploy container images. This step-by-step guide covers the setup of the .gitlab-ci.yaml file, creating a Dockerfile, adding application code, configuring OpenShift, and running the build. Perfect for developers looking to leverage the power of GitLab and OpenShift for efficient container management and deployment.</p></div></div></div>"
    } 
  
]
